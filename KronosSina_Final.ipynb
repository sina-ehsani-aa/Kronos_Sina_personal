{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0875ceca-54ae-42db-9819-b22c28a8628a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "from pullDate_FullPeriod import pull_data , pull_seas \n",
    "from utility import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db5d5cb3-5536-4807-8b0e-b5a597d95548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python local connection to Oracle (herccrt) and Teradata (mosaic)\n",
    "def connect_to_servers():\n",
    "    from config import  herccrt,mosaic, azure\n",
    "    hcrt = herccrt().con()\n",
    "    mos = mosaic().con()\n",
    "    az = azure().con()\n",
    "    return hcrt, mos, az\n",
    "\n",
    "# jupyter notebook settings\n",
    "import warnings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20) # DON't Use None, it will show every row --> resulting in CRASH\n",
    "\n",
    "hcrt, mos, az = connect_to_servers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b773df5-9c06-4cda-8725-ff71d54e3172",
   "metadata": {},
   "source": [
    "## Kronos .Sina Model\n",
    "\n",
    "_SUMMARY_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fb76f2b-dfd5-45f1-a95b-aa360707ca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kronos Model\n",
    "import os\n",
    "from keras import backend as K \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Reshape , BatchNormalization, LSTM, Concatenate, Dense, Activation, Flatten, Conv2D, Conv1D, ConvLSTM2D, Conv3D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, History, ModelCheckpoint\n",
    "import random\n",
    "\n",
    "\n",
    "def kronos_32s_model(para_epochs, para_early_stop, para_model_name, para_sea_len, para_sea_dense, window, train_list, val_list, test_list):\n",
    "        \n",
    "    # Set Random Seed \n",
    "    seed_value = 44\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "\n",
    "    \n",
    "    # extract train/val/test datasets  ** since TF.keras only accepts channels_last we have to move axix **\n",
    "    train_fc, train_sea, train_traf_time, train_output = train_list\n",
    "    val_fc, val_sea, val_traf_time, val_output = val_list\n",
    "    test_fc, test_sea, test_traf_time, test_gold_output = test_list\n",
    "    \n",
    "    # Reshape for Conv3d with channel 1.\n",
    "    train_fc, val_fc, test_fc = train_fc.reshape(list(train_fc.shape)+[1]), val_fc.reshape(list(val_fc.shape)+[1]), test_fc.reshape(list(test_fc.shape)+[1]) \n",
    "    \n",
    "    \n",
    "    # Model Parameters:\n",
    "    batch_size = 100\n",
    "    early_stop = 100\n",
    "    filter_num = 8\n",
    "    rec_act = 'relu'\n",
    "    rec_dropout = 0.1\n",
    "    dropout = 0.1\n",
    "    lstm_size = 64\n",
    "    patience= early_stop\n",
    "    batch_size_nbr = batch_size\n",
    "    Error_Method = 'MC_dropout'\n",
    "    B = batch_size\n",
    "    # 128, 64,64\n",
    "\n",
    "    # Input sizes: \n",
    "    K.clear_session()\n",
    "    input_tensor_closure = Input(shape = (2,7,10,1)) # lets use 3D  with Channel of 1\n",
    "    input_tensor_seas = Input(shape = (window ,para_sea_len))\n",
    "    input_tensor_seas = Input(shape = (para_sea_len))\n",
    "    input_tensor_trrafic_tseries = Input(shape = (window ,2,7,10))\n",
    "\n",
    "    # Feed the TF-t-series to ConvLSTM:\n",
    "    cnn_lstm_tf_time = ConvLSTM2D(filters = filter_num , kernel_size = 3, padding = 'same', data_format = 'channels_first', \n",
    "                                  recurrent_activation = rec_act, recurrent_dropout = rec_dropout, dropout=dropout ,  return_sequences = False)(input_tensor_trrafic_tseries)\n",
    "    cnn_lstm_tf_time = BatchNormalization(axis = 1)(cnn_lstm_tf_time)\n",
    "    # cnn_lstm_tf_time = ConvLSTM2D(filters = filter_num//2 , kernel_size = 2, padding = 'same', data_format = 'channels_first', \n",
    "    #                               recurrent_activation = rec_act, recurrent_dropout = rec_dropout, dropout=dropout ,  return_sequences = False)(cnn_lstm_tf_time)\n",
    "    # cnn_lstm_tf_time = BatchNormalization(axis = 1)(cnn_lstm_tf_time)\n",
    "    cnn_lstm_tf_time = Flatten()(cnn_lstm_tf_time)\n",
    "    cnn_lstm_tf_time = Dense(32, activation=rec_act)(cnn_lstm_tf_time)\n",
    "\n",
    "    # print(cnn_lstm_tf_time)\n",
    "\n",
    "\n",
    "    # # layer for seasonality Using a RNN (Here GRU instead of LSTM)\n",
    "    # rnn_seas = GRU(lstm_size, recurrent_dropout = rec_dropout, dropout=dropout  , return_sequences = False  )(input_tensor_seas)\n",
    "    # # rnn_seas = Bidirectional(GRU(lstm_size//2, recurrent_dropout = rec_dropout, dropout=dropout  , return_sequences = False ))(rnn_seas)\n",
    "    # rnn_seas = BatchNormalization()(rnn_seas)\n",
    "    # rnn_seas = Dense(64, activation=rec_act)(rnn_seas)\n",
    "\n",
    "    # Seas No RNN:\n",
    "    rnn_seas = Dense(128, activation=rec_act)(input_tensor_seas)\n",
    "    rnn_seas = Dense(32, activation=rec_act)(rnn_seas)\n",
    "\n",
    "\n",
    "    # print(rnn_seas)\n",
    "\n",
    "\n",
    "    # layer for FC:\n",
    "    cnn_FC = Conv3D(filters = filter_num , kernel_size = 3, strides = (1, 1, 1), padding = 'same', data_format = 'channels_last')(input_tensor_closure)\n",
    "    cnn_FC = Conv3D(filters = filter_num//2 , kernel_size = 3, strides = (1, 1, 1), padding = 'same', data_format = 'channels_last')(cnn_FC)\n",
    "    cnn_FC = BatchNormalization()(cnn_FC)\n",
    "    cnn_FC = Flatten()(cnn_FC)\n",
    "    cnn_FC = Dense(32, activation=rec_act)(cnn_FC)\n",
    "\n",
    "    # print(cnn_FC)\n",
    "\n",
    "\n",
    "    # concat frac_closure and seasonality and TF_sries\n",
    "    FF_concat_all = Concatenate()([cnn_lstm_tf_time, rnn_seas, cnn_FC])\n",
    "    # FF_concat_all = Dense(256, activation=rec_act)(FF_concat_all)\n",
    "    FF_concat_all = Dense(140, activation=rec_act)(FF_concat_all)\n",
    "\n",
    "    # # Output format:\n",
    "    output_tensor = Reshape((2, 7, 10))(FF_concat_all)\n",
    "\n",
    "    # print(output_tensor)\n",
    "\n",
    "\n",
    "    # Kronos model setting\n",
    "    Kronos_model = Model(inputs = [ input_tensor_closure, input_tensor_seas, input_tensor_trrafic_tseries], outputs = output_tensor)\n",
    "    Kronos_model.compile(loss = 'mean_squared_error', optimizer = 'adam')\n",
    "    \n",
    "    if para_early_stop: # with early stop\n",
    "        \n",
    "        os.chdir(r\"\\\\corpaa.aa.com\\campusshared\\HDQ\\HDQ_REVMGMT_Share\\RMDEPT\")\n",
    "        path = \"BFox/Kronos/Prototype/Output/FullPeriod/\"\n",
    "        market_nm = orig + dest\n",
    "        # if not os.path.exists(path):\n",
    "            # os.mkdir(path)\n",
    "        \n",
    "        # history = History()\n",
    "        metric = 'val_accuracy'\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=patience , restore_best_weights=True)\n",
    "        mc = ModelCheckpoint(path + '//' + market_nm + '_' + f'{para_model_name}.h5', monitor='val_loss', mode='auto', verbose=0, save_best_only=True, save_freq='epoch')\n",
    "        base_hist = Kronos_model.fit([train_fc, train_sea, train_traf_time], train_output, epochs = para_epochs, \n",
    "                                 batch_size = batch_size_nbr, validation_data = ([ val_fc, val_sea, val_traf_time], val_output), verbose = 0, callbacks=[es,mc])\n",
    "    else: # no early stop\n",
    "        base_hist = Kronos_model.fit([train_fc, train_sea, train_traf_time], train_output, epochs = para_epochs, \n",
    "                                 batch_size = batch_size_nbr, validation_data = ([ val_fc, val_sea, val_traf_time], val_output) , verbose = 0)\n",
    "         \n",
    "    \n",
    "    # use the trained model to predict\n",
    "    test_pred = Kronos_model.predict([ test_fc, test_sea, test_traf_time])\n",
    "\n",
    "    return test_pred, base_hist, Kronos_model\n",
    "\n",
    "\n",
    "def test_acc(prediction_results, gold_labels):\n",
    "    results = pd.DataFrame()\n",
    "    test_size = gold_labels.shape[0]\n",
    "\n",
    "    gold_tr_reshaped = gold_labels.reshape(test_size*14,10)\n",
    "    pred_tr_reshaped = prediction_results.reshape(test_size*14,10)\n",
    "\n",
    "    # Gold, top, mid, bot\n",
    "    results['gold_top_tr'] = gold_tr_reshaped[:,:3].sum(1)\n",
    "    results['gold_mid_tr'] = gold_tr_reshaped[:,3:7].sum(1)\n",
    "    results['gold_bot_tr'] = gold_tr_reshaped[:,7:].sum(1)\n",
    "    results['gold_sum_tr'] = gold_tr_reshaped.sum(1)\n",
    "\n",
    "    # Pred Top, Mid, Bot\n",
    "    results['pred_top_tr'] = pred_tr_reshaped[:,:3].sum(1)\n",
    "    results['pred_mid_tr'] = pred_tr_reshaped[:,3:7].sum(1)\n",
    "    results['pred_bot_tr'] = pred_tr_reshaped[:,7:].sum(1)\n",
    "    results['pred_sum_tr'] = pred_tr_reshaped.sum(1)\n",
    "\n",
    "    # all FvT errors\n",
    "    results['top_FvT'] = results['pred_top_tr'] - results['gold_top_tr']\n",
    "    results['mid_FvT'] = results['pred_mid_tr'] - results['gold_mid_tr']\n",
    "    results['bot_FvT'] = results['pred_bot_tr'] - results['gold_bot_tr']\n",
    "    results['sum_FvT'] = results['pred_sum_tr'] - results['gold_sum_tr']\n",
    "\n",
    "    # squared FvT errors for MSE\n",
    "    results['top_FvT_sqr'] = results['top_FvT']**2\n",
    "    results['mid_FvT_sqr'] = results['mid_FvT']**2\n",
    "    results['bot_FvT_sqr'] = results['bot_FvT']**2\n",
    "    results['sum_FvT_sqr'] = results['sum_FvT']**2\n",
    "    \n",
    "    result_sum = { \n",
    "                    \"top_FvT\" : [results['top_FvT'].mean(), results['top_FvT'].std(), results['top_FvT_sqr'].mean()],\n",
    "                    \"mid_FvT\" : [results['mid_FvT'].mean(), results['mid_FvT'].std(), results['mid_FvT_sqr'].mean()],\n",
    "                    \"bot_FvT\" : [results['bot_FvT'].mean(), results['bot_FvT'].std(), results['bot_FvT_sqr'].mean()],\n",
    "                    \"sum_FvT\" : [results['sum_FvT'].mean(), results['sum_FvT'].std(), results['sum_FvT_sqr'].mean()]\n",
    "                }\n",
    "    return results , result_sum\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e79aa35-65f4-4b48-a75a-2e7555f8ea54",
   "metadata": {},
   "source": [
    "## Training:\n",
    "\n",
    "_SUMMARY_\n",
    "\n",
    "Run the model for only one leg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5a7ac59-09ea-480f-a1f7-81df98a3b77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring the warnings\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Ignoring the warnings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "069ffb4a-82be-4cb1-9982-4a1589173226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Flights from DFW to TUS:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "373894d0f1444ef19b9b8ba374668c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------------- ****** DFW-TUS-1) ****** ------------- \n",
      "fcst_start and fcst_end for DFW-TUS at FCST_ID 1 are: 180, 593\n",
      " For market (DFW-TUS-1) , we have 812.0 Pre-Covid data, 418.0 Post-Covid data, and 147.0 future data (from now to one year from now)\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "orig = \"DFW\"\n",
    "dest = \"TUS\"\n",
    "\n",
    "DOW = True\n",
    "\n",
    "# If test_random_masking is False -> Means you have to give it a date for creating \"fake today\"\n",
    "test_random_masking = True\n",
    "test_today = '2022-06-01'\n",
    "\n",
    "yesterday =  datetime.today() - timedelta(days=2)\n",
    "next_year_today = datetime.today() + timedelta(days=365)\n",
    "\n",
    "pull_start = '2017-09-01'\n",
    "pull_end = next_year_today.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Pre: pre-covid period, used for train and validation\n",
    "Pre_start, Pre_end = '2017-09-01', '2020-01-30'\n",
    "# Post: post-covid period, used for test\n",
    "Post_start, Post_end = '2021-07-01',  yesterday.strftime(\"%Y-%m-%d\")\n",
    "# Train on All:\n",
    "\n",
    "# Future: Today till one year in future:\n",
    "future_start , future_end = datetime.today().strftime(\"%Y-%m-%d\") ,   next_year_today.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "new_market = False # change this to True if it is a new market\n",
    "ulcc_list = ['NK','SY','F9'] # Spirit SunCountry Frontier \n",
    "\n",
    "# Extracting for Seas:\n",
    "sea_col_fcst = ['week_x', 'week_y', 'forecastDayOfWeek','avgrasm','dowavgrasm','seats_AA_fcst', 'holiday', 'forecastId'] #+ forecastDayOfWeek, FCST\n",
    "sea_col_Cap = ['week_x', 'week_y','dow_x', 'dow_y', 'avgrasm','seats_AA_fcst','seats_OA_fcst','seats_ulcc_fcst' , 'seats_AA' , 'seats_OA' , 'seats_ulcc']\n",
    "sea_col = ['week_x', 'week_y', 'dow_x', 'dow_y','avgrasm','dowavgrasm']\n",
    "\n",
    "# Data reshaping parameters:\n",
    "train_val_percentage = .9\n",
    "time_series = False\n",
    "seasenality_one_dimension = False \n",
    "window = 0 \n",
    "\n",
    "# Model parameters;\n",
    "epochs = 150\n",
    "early_stop = 10\n",
    "sea_dense = 128\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "kronos_3_timeseries = defaultdict()\n",
    "\n",
    "hcrt, mos, az = connect_to_servers()\n",
    "\n",
    "\n",
    "print(f\" Flights from {orig} to {dest}:\")\n",
    "# Pull all the FCSTs\n",
    "fcst_id_df = get_fcst_given_leg(orig, dest, hcrt )   \n",
    "\n",
    "# Pull OAG:\n",
    "oag_df = get_oag_data(orig, dest, pull_start, pull_end, ulcc_list, mos)\n",
    "\n",
    "#Pull prdMaps:\n",
    "prdMaps = get_prdMaps(orig, dest, hcrt)\n",
    "\n",
    "\n",
    "# Processing: OAG Per Day:\n",
    "oag_kl_total_Per_Day_and_AA = oag_per_day(oag_df)\n",
    "\n",
    "all_tensors = [[] for i in range(12)] # 4 (Number of tensors - TF (output) , FC, SEA, TF_Time) * 3 (Train, Val, Test) \n",
    "\n",
    "for _,_ , fcst_id , fcst_start , fcst_end in tqdm(fcst_id_df.values):\n",
    "\n",
    "    hcrt, mos, az = connect_to_servers()\n",
    "\n",
    "    print(f\" ------------- ****** {orig}-{dest}-{fcst_id}) ****** ------------- \")\n",
    "\n",
    "    # print( fcst_id , fas, adf )\n",
    "    print( f\"fcst_start and fcst_end for {orig}-{dest} at FCST_ID {fcst_id} are: {fcst_start}, {fcst_end}\") \n",
    "\n",
    "    #  Processing: OAG per FCST:\n",
    "    oag_kl =  oag_per_fcst(oag_df, fcst_start, fcst_end )\n",
    "\n",
    "    # Merge and Normalize: OAG per FCST and OAG per Day:\n",
    "    oag_kl_fcst_total = pd.merge(oag_kl,oag_kl_total_Per_Day_and_AA ,on = \"adj_dep_date\", how='left',suffixes=('_fcst', '_day'))\n",
    "    oag_kl_fcst_total = normalize_oag_kl_fcst_total(oag_kl_fcst_total)\n",
    "\n",
    "\n",
    "    # Pull data from the file: pullData_FullPeriod.py\n",
    "    df = pull_data(orig,dest,fcst_id,new_market)\n",
    "    df = pull_seas(df, orig, dest)\n",
    "\n",
    "    if len(df) < 100:\n",
    "        print(f\"insufficent data for market ({orig}-{dest}-{fcst_id}), IGNORED\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    df['flightDepartureDate'] = pd.to_datetime(df['flightDepartureDate'], format='%Y/%m/%d')\n",
    "\n",
    "    # Merge new features (including the total day seats) into current Kronos dataset by dep_date\n",
    "    df = pd.merge(df,oag_kl_fcst_total, left_on=['flightDepartureDate'],\\\n",
    "          right_on=['adj_dep_date'], how='left')\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # processing: Group and pad the DF:\n",
    "    post = group_and_pad(df)\n",
    "\n",
    "    # Cut the 'post' data into Pre-covid and Post-covid parts\n",
    "    Data_PRE = post[ (post['flightDepartureDate']>=Pre_start) & (post['flightDepartureDate']<=Pre_end) ]\n",
    "    Data_POST = post[ (post['flightDepartureDate']>=Post_start) & (post['flightDepartureDate']<=Post_end) ]\n",
    "    Data_FUTURE = post[ (post['flightDepartureDate']>=future_start) & (post['flightDepartureDate']<=future_end) ]\n",
    "    \n",
    "    Data_PRE = Data_PRE.reset_index(drop = True)\n",
    "    Data_POST = Data_POST.reset_index(drop = True)\n",
    "    Data_FUTURE = Data_FUTURE.reset_index(drop = True)\n",
    "    \n",
    "    if test_today and not test_random_masking:\n",
    "        if DOW:\n",
    "            split_date = (datetime.strptime(test_today,'%Y-%m-%d') - timedelta(days=window*7)).strftime(\"%Y-%m-%d\")\n",
    "        else:\n",
    "            split_date = (datetime.strptime(test_today,'%Y-%m-%d') - timedelta(days=window)).strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        Data_PAST = pd.concat([Data_PRE,Data_POST])\n",
    "        Data_PAST =  Data_PAST[(Data_PAST['flightDepartureDate']>=Post_start)]\n",
    "        \n",
    "        Data_PRE = Data_PAST[Data_PAST['flightDepartureDate']<split_date]\n",
    "        Data_POST = Data_PAST[Data_PAST['flightDepartureDate']>=split_date]     \n",
    "        \n",
    "        Data_PRE = Data_PRE.reset_index(drop = True)\n",
    "        Data_POST = Data_POST.reset_index(drop = True)\n",
    "    \n",
    "    print(f\" For market ({orig}-{dest}-{fcst_id}) , we have {Data_PRE.shape[0]/14} Pre-Covid data, {Data_POST.shape[0]/14} Post-Covid data, and {Data_FUTURE.shape[0]/14} future data (from now to one year from now)\")\n",
    "\n",
    "    # TODO: This can be edited later, so we use the POST data to train....\n",
    "    if len(Data_PRE) <= len(Data_POST):\n",
    "        print(f\" *** The Pre Covid data is less than the post covid data, so we ignore {orig}-{dest}-{fcst_id} market ***\")\n",
    "        continue\n",
    "\n",
    "#         # TODO: When merging models together this can be useefull for training porposes, but for now not usefull.\n",
    "    if len(Data_FUTURE)//14 <= 5 * 7:\n",
    "        print(f\" *** There is no future flights for the {orig}-{dest}-{fcst_id} market, so ignored! *** \")\n",
    "        continue\n",
    "\n",
    "    if any([Data_PRE.shape[0]/14 <= 10 * 7 , Data_POST.shape[0]/14 <= 10 * 7]):\n",
    "        print(f\" *** Low amount of data for either PRE, or POST of {orig}-{dest}-{fcst_id} market, so ignored! *** \")\n",
    "        continue\n",
    "    \n",
    "    # MErge FCSTs:\n",
    "\n",
    "    # Train Kronos 2 (No Additional Feat (using sea_col)):\n",
    "    use_channels = True\n",
    "    seasenality_one_dimension = True\n",
    "    DOW= True\n",
    "    FC_time_series = False\n",
    "    traffic_time_series = True \n",
    "    window = 10\n",
    "    test_random_masking = False\n",
    "    # test_today = '2022-07-01'\n",
    "    train, val, test = get_train_test_samples2(Data_PRE, Data_POST, Data_FUTURE, sea_col_fcst, prdMaps , DOW= DOW , train_val_percentage = train_val_percentage ,  FC_time_series = FC_time_series , traffic_time_series = traffic_time_series,  use_channels = use_channels , seasenality_one_dimension = seasenality_one_dimension ,  window = window, test_random_masking = test_random_masking, test_today = test_today )\n",
    "    break\n",
    "#     [all_tensors[i].append(item) for i,item in enumerate( train + val +test)]\n",
    "\n",
    "# if len(all_tensors[0]) == 0:\n",
    "#     print(f\"No data for {orig}-{dest} -> No Model!\")\n",
    "\n",
    "# # Now concatinate and shuffle the data:\n",
    "# train = [np.concatenate(all_tensors[i]) for i in range(0,4)] \n",
    "# val = [np.concatenate(all_tensors[i]) for i in range(4,8)] \n",
    "# test = [np.concatenate(all_tensors[i]) for i in range(8,12)] \n",
    "\n",
    "# # Shuffle:\n",
    "# train = shuffle(train[0],train[1],train[2],train[3])\n",
    "# val = shuffle(val[0],val[1],val[2],val[3])\n",
    "\n",
    "\n",
    "# kronos32s_test_results3 , kronos32s_hist3,  kronos32s_model3 = kronos_32s_model(para_epochs = 500, para_early_stop =  True, para_model_name = 'kronos32s', para_sea_len = len(sea_col_fcst), para_sea_dense =  160, window = window ,\n",
    "#          train_list = train , val_list = val, test_list = test)\n",
    "\n",
    "# _ , results_summary = test_acc(kronos32s_test_results3, test[3])\n",
    "\n",
    "# kronos_3_timeseries[f\"{orig}-{dest}\"] = results_summary\n",
    "\n",
    "# print(f\"val plot {orig}-{dest}\")\n",
    "# plt.plot(kronos32s_hist3.history['loss'])\n",
    "# plt.plot(kronos32s_hist3.history['val_loss'])\n",
    "# plt.title('loss')\n",
    "# plt.ylabel('MSE')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train','val'] , loc = \"upper left\")\n",
    "# plt.show()\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adc77370-52e0-4d57-b23e-f434110b6cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>snapshotDate</th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "      <th>forecastId</th>\n",
       "      <th>forecastDepartureDate</th>\n",
       "      <th>forecastDayOfWeek</th>\n",
       "      <th>poolCode</th>\n",
       "      <th>cabinCode</th>\n",
       "      <th>forecastPeriod</th>\n",
       "      <th>localFlowIndicator</th>\n",
       "      <th>flightDepartureDate</th>\n",
       "      <th>fracClosure_1</th>\n",
       "      <th>fracClosure_2</th>\n",
       "      <th>fracClosure_3</th>\n",
       "      <th>fracClosure_4</th>\n",
       "      <th>fracClosure_5</th>\n",
       "      <th>fracClosure_6</th>\n",
       "      <th>fracClosure_7</th>\n",
       "      <th>fracClosure_8</th>\n",
       "      <th>fracClosure_9</th>\n",
       "      <th>fracClosure_10</th>\n",
       "      <th>trafficActual_1</th>\n",
       "      <th>trafficActual_2</th>\n",
       "      <th>trafficActual_3</th>\n",
       "      <th>trafficActual_4</th>\n",
       "      <th>trafficActual_5</th>\n",
       "      <th>trafficActual_6</th>\n",
       "      <th>trafficActual_7</th>\n",
       "      <th>trafficActual_8</th>\n",
       "      <th>trafficActual_9</th>\n",
       "      <th>trafficActual_10</th>\n",
       "      <th>trafficActualAadv_1</th>\n",
       "      <th>trafficActualAadv_2</th>\n",
       "      <th>trafficActualAadv_3</th>\n",
       "      <th>trafficActualAadv_4</th>\n",
       "      <th>trafficActualAadv_5</th>\n",
       "      <th>trafficActualAadv_6</th>\n",
       "      <th>trafficActualAadv_7</th>\n",
       "      <th>trafficActualAadv_8</th>\n",
       "      <th>trafficActualAadv_9</th>\n",
       "      <th>trafficActualAadv_10</th>\n",
       "      <th>holiday</th>\n",
       "      <th>H1</th>\n",
       "      <th>H2</th>\n",
       "      <th>H3</th>\n",
       "      <th>HL</th>\n",
       "      <th>weekNumber</th>\n",
       "      <th>week_x</th>\n",
       "      <th>week_y</th>\n",
       "      <th>dow_x</th>\n",
       "      <th>dow_y</th>\n",
       "      <th>avgtraffic</th>\n",
       "      <th>avgtrafficopenness</th>\n",
       "      <th>avgrasm</th>\n",
       "      <th>dowavgtraffic</th>\n",
       "      <th>dowavgtrafficopenness</th>\n",
       "      <th>dowavgrasm</th>\n",
       "      <th>adj_dep_date</th>\n",
       "      <th>fcst_start</th>\n",
       "      <th>fcst_end</th>\n",
       "      <th>seats_AA_fcst</th>\n",
       "      <th>seats_OA_fcst</th>\n",
       "      <th>seats_ulcc_fcst</th>\n",
       "      <th>seats_All_fcst</th>\n",
       "      <th>flt_ct_AA_fcst</th>\n",
       "      <th>flt_ct_OA_fcst</th>\n",
       "      <th>flt_ct_ulcc_fcst</th>\n",
       "      <th>flt_ct_All_fcst</th>\n",
       "      <th>asm_AA_fcst</th>\n",
       "      <th>asm_All_fcst</th>\n",
       "      <th>seats_AA</th>\n",
       "      <th>seats_OA</th>\n",
       "      <th>seats_ulcc</th>\n",
       "      <th>seats_All</th>\n",
       "      <th>flt_ct_AA</th>\n",
       "      <th>flt_ct_OA</th>\n",
       "      <th>flt_ct_ulcc</th>\n",
       "      <th>flt_ct_All</th>\n",
       "      <th>asm_AA</th>\n",
       "      <th>asm_All</th>\n",
       "      <th>groupID</th>\n",
       "      <th>fullHistory</th>\n",
       "      <th>real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2022-11-18</td>\n",
       "      <td>DFW</td>\n",
       "      <td>TUS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>H3</td>\n",
       "      <td>Y</td>\n",
       "      <td>1.0</td>\n",
       "      <td>F</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>180.0</td>\n",
       "      <td>593.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2022-11-18</td>\n",
       "      <td>DFW</td>\n",
       "      <td>TUS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>H3</td>\n",
       "      <td>Y</td>\n",
       "      <td>2.0</td>\n",
       "      <td>F</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.964</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.886688</td>\n",
       "      <td>0.182648</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.086</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>180.0</td>\n",
       "      <td>593.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2022-11-18</td>\n",
       "      <td>DFW</td>\n",
       "      <td>TUS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>H3</td>\n",
       "      <td>Y</td>\n",
       "      <td>3.0</td>\n",
       "      <td>F</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.886</td>\n",
       "      <td>0.954</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.886688</td>\n",
       "      <td>0.182648</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.086</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>180.0</td>\n",
       "      <td>593.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2022-11-18</td>\n",
       "      <td>DFW</td>\n",
       "      <td>TUS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>H3</td>\n",
       "      <td>Y</td>\n",
       "      <td>4.0</td>\n",
       "      <td>F</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.886688</td>\n",
       "      <td>0.182648</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.086</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>180.0</td>\n",
       "      <td>593.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2022-11-18</td>\n",
       "      <td>DFW</td>\n",
       "      <td>TUS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>H3</td>\n",
       "      <td>Y</td>\n",
       "      <td>5.0</td>\n",
       "      <td>F</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.979</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.886688</td>\n",
       "      <td>0.182648</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.086</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>180.0</td>\n",
       "      <td>593.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2022-11-18</td>\n",
       "      <td>DFW</td>\n",
       "      <td>TUS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>H3</td>\n",
       "      <td>Y</td>\n",
       "      <td>6.0</td>\n",
       "      <td>F</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.913</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>11.2</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.886688</td>\n",
       "      <td>0.182648</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.086</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>180.0</td>\n",
       "      <td>593.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2022-11-18</td>\n",
       "      <td>DFW</td>\n",
       "      <td>TUS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>H3</td>\n",
       "      <td>Y</td>\n",
       "      <td>7.0</td>\n",
       "      <td>F</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.988</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.886688</td>\n",
       "      <td>0.182648</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.086</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>180.0</td>\n",
       "      <td>593.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2022-11-18</td>\n",
       "      <td>DFW</td>\n",
       "      <td>TUS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>H3</td>\n",
       "      <td>Y</td>\n",
       "      <td>1.0</td>\n",
       "      <td>L</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>180.0</td>\n",
       "      <td>593.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>2022-11-18</td>\n",
       "      <td>DFW</td>\n",
       "      <td>TUS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>H3</td>\n",
       "      <td>Y</td>\n",
       "      <td>2.0</td>\n",
       "      <td>L</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.886688</td>\n",
       "      <td>0.182648</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.086</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>180.0</td>\n",
       "      <td>593.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>2022-11-18</td>\n",
       "      <td>DFW</td>\n",
       "      <td>TUS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>H3</td>\n",
       "      <td>Y</td>\n",
       "      <td>3.0</td>\n",
       "      <td>L</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.886688</td>\n",
       "      <td>0.182648</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.086</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>180.0</td>\n",
       "      <td>593.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2022-11-18</td>\n",
       "      <td>DFW</td>\n",
       "      <td>TUS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>H3</td>\n",
       "      <td>Y</td>\n",
       "      <td>4.0</td>\n",
       "      <td>L</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.886688</td>\n",
       "      <td>0.182648</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.086</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>180.0</td>\n",
       "      <td>593.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2022-11-18</td>\n",
       "      <td>DFW</td>\n",
       "      <td>TUS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>H3</td>\n",
       "      <td>Y</td>\n",
       "      <td>5.0</td>\n",
       "      <td>L</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.886688</td>\n",
       "      <td>0.182648</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.086</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>180.0</td>\n",
       "      <td>593.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2022-11-18</td>\n",
       "      <td>DFW</td>\n",
       "      <td>TUS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>H3</td>\n",
       "      <td>Y</td>\n",
       "      <td>6.0</td>\n",
       "      <td>L</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.835</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.886688</td>\n",
       "      <td>0.182648</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.086</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>180.0</td>\n",
       "      <td>593.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2022-11-18</td>\n",
       "      <td>DFW</td>\n",
       "      <td>TUS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>H3</td>\n",
       "      <td>Y</td>\n",
       "      <td>7.0</td>\n",
       "      <td>L</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.989</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.886688</td>\n",
       "      <td>0.182648</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.086</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>180.0</td>\n",
       "      <td>593.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.459821</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   snapshotDate origin destination  forecastId forecastDepartureDate  \\\n",
       "70   2022-11-18    DFW         TUS         1.0            2022-11-23   \n",
       "71   2022-11-18    DFW         TUS         1.0            2022-11-23   \n",
       "72   2022-11-18    DFW         TUS         1.0            2022-11-23   \n",
       "73   2022-11-18    DFW         TUS         1.0            2022-11-23   \n",
       "74   2022-11-18    DFW         TUS         1.0            2022-11-23   \n",
       "75   2022-11-18    DFW         TUS         1.0            2022-11-23   \n",
       "76   2022-11-18    DFW         TUS         1.0            2022-11-23   \n",
       "77   2022-11-18    DFW         TUS         1.0            2022-11-23   \n",
       "78   2022-11-18    DFW         TUS         1.0            2022-11-23   \n",
       "79   2022-11-18    DFW         TUS         1.0            2022-11-23   \n",
       "80   2022-11-18    DFW         TUS         1.0            2022-11-23   \n",
       "81   2022-11-18    DFW         TUS         1.0            2022-11-23   \n",
       "82   2022-11-18    DFW         TUS         1.0            2022-11-23   \n",
       "83   2022-11-18    DFW         TUS         1.0            2022-11-23   \n",
       "\n",
       "    forecastDayOfWeek poolCode cabinCode  forecastPeriod localFlowIndicator  \\\n",
       "70                3.0       H3         Y             1.0                  F   \n",
       "71                3.0       H3         Y             2.0                  F   \n",
       "72                3.0       H3         Y             3.0                  F   \n",
       "73                3.0       H3         Y             4.0                  F   \n",
       "74                3.0       H3         Y             5.0                  F   \n",
       "75                3.0       H3         Y             6.0                  F   \n",
       "76                3.0       H3         Y             7.0                  F   \n",
       "77                3.0       H3         Y             1.0                  L   \n",
       "78                3.0       H3         Y             2.0                  L   \n",
       "79                3.0       H3         Y             3.0                  L   \n",
       "80                3.0       H3         Y             4.0                  L   \n",
       "81                3.0       H3         Y             5.0                  L   \n",
       "82                3.0       H3         Y             6.0                  L   \n",
       "83                3.0       H3         Y             7.0                  L   \n",
       "\n",
       "   flightDepartureDate  fracClosure_1  fracClosure_2  fracClosure_3  \\\n",
       "70          2022-11-23          0.000          0.000          0.000   \n",
       "71          2022-11-23          0.000          0.019          0.051   \n",
       "72          2022-11-23          0.000          0.014          0.041   \n",
       "73          2022-11-23          0.000          0.012          0.041   \n",
       "74          2022-11-23          0.000          0.000          0.035   \n",
       "75          2022-11-23          0.000          0.002          0.011   \n",
       "76          2022-11-23          0.001          0.015          0.024   \n",
       "77          2022-11-23          0.000          0.000          0.000   \n",
       "78          2022-11-23          0.000          0.000          0.000   \n",
       "79          2022-11-23          0.000          0.000          0.000   \n",
       "80          2022-11-23          0.000          0.000          0.000   \n",
       "81          2022-11-23          0.000          0.000          0.000   \n",
       "82          2022-11-23          0.000          0.000          0.000   \n",
       "83          2022-11-23          0.000          0.000          0.000   \n",
       "\n",
       "    fracClosure_4  fracClosure_5  fracClosure_6  fracClosure_7  fracClosure_8  \\\n",
       "70          0.000          0.000          0.000          0.000          0.000   \n",
       "71          0.070          0.577          0.632          0.918          0.964   \n",
       "72          0.058          0.069          0.107          0.775          0.886   \n",
       "73          0.060          0.099          0.242          0.290          0.411   \n",
       "74          0.041          0.062          0.136          0.337          0.446   \n",
       "75          0.036          0.088          0.199          0.427          0.913   \n",
       "76          0.071          0.220          0.271          0.354          0.766   \n",
       "77          0.000          0.000          0.000          0.000          0.000   \n",
       "78          0.000          0.000          1.000          1.000          1.000   \n",
       "79          0.000          0.000          0.000          1.000          1.000   \n",
       "80          0.000          0.000          0.000          0.000          1.000   \n",
       "81          0.000          0.000          0.000          0.000          1.000   \n",
       "82          0.000          0.000          0.000          0.835          1.000   \n",
       "83          0.000          0.000          0.000          0.541          0.981   \n",
       "\n",
       "    fracClosure_9  fracClosure_10  trafficActual_1  trafficActual_2  \\\n",
       "70          0.000           0.000             -1.0             -1.0   \n",
       "71          1.000           1.000              0.0              0.0   \n",
       "72          0.954           1.000              0.0              0.0   \n",
       "73          0.959           0.999              0.0              0.0   \n",
       "74          0.979           1.000              0.8              0.0   \n",
       "75          1.000           1.000              0.0              0.0   \n",
       "76          0.988           1.000              0.0              0.0   \n",
       "77          0.000           0.000             -1.0             -1.0   \n",
       "78          1.000           1.000              0.0              0.0   \n",
       "79          1.000           1.000              0.0              0.0   \n",
       "80          1.000           1.000              0.0              0.0   \n",
       "81          1.000           1.000              0.0              0.0   \n",
       "82          1.000           1.000              0.0              0.0   \n",
       "83          0.989           1.000              0.0              0.0   \n",
       "\n",
       "    trafficActual_3  trafficActual_4  trafficActual_5  trafficActual_6  \\\n",
       "70             -1.0             -1.0             -1.0             -1.0   \n",
       "71              0.0              0.8              0.0              3.2   \n",
       "72              0.8              0.8              0.8              1.6   \n",
       "73              0.0              0.0              0.8              0.0   \n",
       "74              0.0              0.0              5.6              0.8   \n",
       "75              1.6              1.6              2.4             11.2   \n",
       "76              0.0              0.8              0.0              0.0   \n",
       "77             -1.0             -1.0             -1.0             -1.0   \n",
       "78              0.0              0.0              0.8              0.0   \n",
       "79              0.0              0.0              0.0              1.6   \n",
       "80              0.0              0.8              0.0              0.8   \n",
       "81              0.0              0.0              0.0              0.0   \n",
       "82              0.0              0.0              0.0              0.0   \n",
       "83              0.0              0.0              0.0              0.0   \n",
       "\n",
       "    trafficActual_7  trafficActual_8  trafficActual_9  trafficActual_10  \\\n",
       "70             -1.0             -1.0             -1.0              -1.0   \n",
       "71              0.8              0.0              0.0               0.0   \n",
       "72              0.0              4.0              2.4               0.0   \n",
       "73              2.4              3.2              1.6               0.0   \n",
       "74              6.4              4.8              1.6               0.0   \n",
       "75              5.6              0.0              0.0               0.0   \n",
       "76              1.6              0.0              0.0               0.0   \n",
       "77             -1.0             -1.0             -1.0              -1.0   \n",
       "78              0.0              0.0              1.6               0.0   \n",
       "79              0.0              0.0              0.0               0.0   \n",
       "80              0.0              0.0              0.8               0.0   \n",
       "81              2.4              0.0              0.0               0.0   \n",
       "82              0.0              0.0              7.2               0.0   \n",
       "83              0.0              0.0              0.0               0.0   \n",
       "\n",
       "    trafficActualAadv_1  trafficActualAadv_2  trafficActualAadv_3  \\\n",
       "70                 -1.0                 -1.0                 -1.0   \n",
       "71                  0.0                  0.0                  0.0   \n",
       "72                  0.0                  0.0                  0.0   \n",
       "73                  0.0                  0.0                  0.0   \n",
       "74                  0.0                  0.0                  0.0   \n",
       "75                  0.0                  0.0                  0.0   \n",
       "76                  0.0                  0.0                  0.0   \n",
       "77                 -1.0                 -1.0                 -1.0   \n",
       "78                  0.0                  0.0                  0.0   \n",
       "79                  0.0                  0.0                  0.0   \n",
       "80                  0.0                  0.0                  0.0   \n",
       "81                  0.0                  0.0                  0.0   \n",
       "82                  0.0                  0.0                  0.0   \n",
       "83                  0.0                  0.0                  0.0   \n",
       "\n",
       "    trafficActualAadv_4  trafficActualAadv_5  trafficActualAadv_6  \\\n",
       "70                 -1.0                 -1.0                 -1.0   \n",
       "71                  0.0                  0.0                  0.0   \n",
       "72                  0.0                  0.0                  0.0   \n",
       "73                  0.0                  0.0                  0.0   \n",
       "74                  0.0                  0.0                  0.0   \n",
       "75                  0.0                  0.8                  0.0   \n",
       "76                  0.0                  0.0                  0.0   \n",
       "77                 -1.0                 -1.0                 -1.0   \n",
       "78                  0.0                  0.0                  0.0   \n",
       "79                  0.0                  0.0                  0.0   \n",
       "80                  0.0                  0.0                  0.0   \n",
       "81                  0.0                  0.0                  0.0   \n",
       "82                  0.0                  0.0                  0.0   \n",
       "83                  0.0                  0.0                  0.0   \n",
       "\n",
       "    trafficActualAadv_7  trafficActualAadv_8  trafficActualAadv_9  \\\n",
       "70                 -1.0                 -1.0                 -1.0   \n",
       "71                  0.0                  0.0                  0.0   \n",
       "72                  0.0                  0.0                  0.0   \n",
       "73                  0.0                  0.0                  0.0   \n",
       "74                  0.0                  0.0                  0.0   \n",
       "75                  0.0                  0.0                  0.0   \n",
       "76                  0.0                  0.0                  0.0   \n",
       "77                 -1.0                 -1.0                 -1.0   \n",
       "78                  0.0                  0.0                  1.6   \n",
       "79                  0.0                  0.0                  0.0   \n",
       "80                  0.0                  0.0                  0.8   \n",
       "81                  0.0                  0.0                  0.0   \n",
       "82                  0.0                  0.0                  4.8   \n",
       "83                  0.0                  0.0                  0.0   \n",
       "\n",
       "    trafficActualAadv_10  holiday   H1   H2   H3   HL  weekNumber    week_x  \\\n",
       "70                  -1.0      0.0  0.0  0.0  0.0  0.0         0.0  0.000000   \n",
       "71                   0.0      1.0  0.0  0.0  1.0  0.0        47.0  0.886688   \n",
       "72                   0.0      1.0  0.0  0.0  1.0  0.0        47.0  0.886688   \n",
       "73                   0.0      1.0  0.0  0.0  1.0  0.0        47.0  0.886688   \n",
       "74                   0.0      1.0  0.0  0.0  1.0  0.0        47.0  0.886688   \n",
       "75                   0.0      1.0  0.0  0.0  1.0  0.0        47.0  0.886688   \n",
       "76                   0.0      1.0  0.0  0.0  1.0  0.0        47.0  0.886688   \n",
       "77                  -1.0      0.0  0.0  0.0  0.0  0.0         0.0  0.000000   \n",
       "78                   0.0      1.0  0.0  0.0  1.0  0.0        47.0  0.886688   \n",
       "79                   0.0      1.0  0.0  0.0  1.0  0.0        47.0  0.886688   \n",
       "80                   0.0      1.0  0.0  0.0  1.0  0.0        47.0  0.886688   \n",
       "81                   0.0      1.0  0.0  0.0  1.0  0.0        47.0  0.886688   \n",
       "82                   0.0      1.0  0.0  0.0  1.0  0.0        47.0  0.886688   \n",
       "83                   0.0      1.0  0.0  0.0  1.0  0.0        47.0  0.886688   \n",
       "\n",
       "      week_y  dow_x  dow_y  avgtraffic  avgtrafficopenness  avgrasm  \\\n",
       "70  0.000000  0.000  0.000       0.000               0.000    0.000   \n",
       "71  0.182648  0.381  0.987       0.168               0.103    0.367   \n",
       "72  0.182648  0.381  0.987       0.168               0.103    0.367   \n",
       "73  0.182648  0.381  0.987       0.168               0.103    0.367   \n",
       "74  0.182648  0.381  0.987       0.168               0.103    0.367   \n",
       "75  0.182648  0.381  0.987       0.168               0.103    0.367   \n",
       "76  0.182648  0.381  0.987       0.168               0.103    0.367   \n",
       "77  0.000000  0.000  0.000       0.000               0.000    0.000   \n",
       "78  0.182648  0.381  0.987       0.686               0.599    0.367   \n",
       "79  0.182648  0.381  0.987       0.686               0.599    0.367   \n",
       "80  0.182648  0.381  0.987       0.686               0.599    0.367   \n",
       "81  0.182648  0.381  0.987       0.686               0.599    0.367   \n",
       "82  0.182648  0.381  0.987       0.686               0.599    0.367   \n",
       "83  0.182648  0.381  0.987       0.686               0.599    0.367   \n",
       "\n",
       "    dowavgtraffic  dowavgtrafficopenness  dowavgrasm adj_dep_date  fcst_start  \\\n",
       "70          0.000                   0.00       0.000   2022-11-23       180.0   \n",
       "71          0.064                   0.00       0.086   2022-11-23       180.0   \n",
       "72          0.064                   0.00       0.086   2022-11-23       180.0   \n",
       "73          0.064                   0.00       0.086   2022-11-23       180.0   \n",
       "74          0.064                   0.00       0.086   2022-11-23       180.0   \n",
       "75          0.064                   0.00       0.086   2022-11-23       180.0   \n",
       "76          0.064                   0.00       0.086   2022-11-23       180.0   \n",
       "77          0.000                   0.00       0.000   2022-11-23       180.0   \n",
       "78          0.408                   0.32       0.086   2022-11-23       180.0   \n",
       "79          0.408                   0.32       0.086   2022-11-23       180.0   \n",
       "80          0.408                   0.32       0.086   2022-11-23       180.0   \n",
       "81          0.408                   0.32       0.086   2022-11-23       180.0   \n",
       "82          0.408                   0.32       0.086   2022-11-23       180.0   \n",
       "83          0.408                   0.32       0.086   2022-11-23       180.0   \n",
       "\n",
       "    fcst_end  seats_AA_fcst  seats_OA_fcst  seats_ulcc_fcst  seats_All_fcst  \\\n",
       "70     593.0       0.459821            0.0              0.0        0.459821   \n",
       "71     593.0       0.459821            0.0              0.0        0.459821   \n",
       "72     593.0       0.459821            0.0              0.0        0.459821   \n",
       "73     593.0       0.459821            0.0              0.0        0.459821   \n",
       "74     593.0       0.459821            0.0              0.0        0.459821   \n",
       "75     593.0       0.459821            0.0              0.0        0.459821   \n",
       "76     593.0       0.459821            0.0              0.0        0.459821   \n",
       "77     593.0       0.459821            0.0              0.0        0.459821   \n",
       "78     593.0       0.459821            0.0              0.0        0.459821   \n",
       "79     593.0       0.459821            0.0              0.0        0.459821   \n",
       "80     593.0       0.459821            0.0              0.0        0.459821   \n",
       "81     593.0       0.459821            0.0              0.0        0.459821   \n",
       "82     593.0       0.459821            0.0              0.0        0.459821   \n",
       "83     593.0       0.459821            0.0              0.0        0.459821   \n",
       "\n",
       "    flt_ct_AA_fcst  flt_ct_OA_fcst  flt_ct_ulcc_fcst  flt_ct_All_fcst  \\\n",
       "70             0.0             0.0               0.0              0.0   \n",
       "71             0.0             0.0               0.0              0.0   \n",
       "72             0.0             0.0               0.0              0.0   \n",
       "73             0.0             0.0               0.0              0.0   \n",
       "74             0.0             0.0               0.0              0.0   \n",
       "75             0.0             0.0               0.0              0.0   \n",
       "76             0.0             0.0               0.0              0.0   \n",
       "77             0.0             0.0               0.0              0.0   \n",
       "78             0.0             0.0               0.0              0.0   \n",
       "79             0.0             0.0               0.0              0.0   \n",
       "80             0.0             0.0               0.0              0.0   \n",
       "81             0.0             0.0               0.0              0.0   \n",
       "82             0.0             0.0               0.0              0.0   \n",
       "83             0.0             0.0               0.0              0.0   \n",
       "\n",
       "    asm_AA_fcst  asm_All_fcst  seats_AA  seats_OA  seats_ulcc  seats_All  \\\n",
       "70     0.459821      0.459821  0.711297       0.0         0.0   0.711297   \n",
       "71     0.459821      0.459821  0.711297       0.0         0.0   0.711297   \n",
       "72     0.459821      0.459821  0.711297       0.0         0.0   0.711297   \n",
       "73     0.459821      0.459821  0.711297       0.0         0.0   0.711297   \n",
       "74     0.459821      0.459821  0.711297       0.0         0.0   0.711297   \n",
       "75     0.459821      0.459821  0.711297       0.0         0.0   0.711297   \n",
       "76     0.459821      0.459821  0.711297       0.0         0.0   0.711297   \n",
       "77     0.459821      0.459821  0.711297       0.0         0.0   0.711297   \n",
       "78     0.459821      0.459821  0.711297       0.0         0.0   0.711297   \n",
       "79     0.459821      0.459821  0.711297       0.0         0.0   0.711297   \n",
       "80     0.459821      0.459821  0.711297       0.0         0.0   0.711297   \n",
       "81     0.459821      0.459821  0.711297       0.0         0.0   0.711297   \n",
       "82     0.459821      0.459821  0.711297       0.0         0.0   0.711297   \n",
       "83     0.459821      0.459821  0.711297       0.0         0.0   0.711297   \n",
       "\n",
       "    flt_ct_AA  flt_ct_OA  flt_ct_ulcc  flt_ct_All    asm_AA   asm_All  \\\n",
       "70   0.571429        0.0          0.0    0.571429  0.711297  0.711297   \n",
       "71   0.571429        0.0          0.0    0.571429  0.711297  0.711297   \n",
       "72   0.571429        0.0          0.0    0.571429  0.711297  0.711297   \n",
       "73   0.571429        0.0          0.0    0.571429  0.711297  0.711297   \n",
       "74   0.571429        0.0          0.0    0.571429  0.711297  0.711297   \n",
       "75   0.571429        0.0          0.0    0.571429  0.711297  0.711297   \n",
       "76   0.571429        0.0          0.0    0.571429  0.711297  0.711297   \n",
       "77   0.571429        0.0          0.0    0.571429  0.711297  0.711297   \n",
       "78   0.571429        0.0          0.0    0.571429  0.711297  0.711297   \n",
       "79   0.571429        0.0          0.0    0.571429  0.711297  0.711297   \n",
       "80   0.571429        0.0          0.0    0.571429  0.711297  0.711297   \n",
       "81   0.571429        0.0          0.0    0.571429  0.711297  0.711297   \n",
       "82   0.571429        0.0          0.0    0.571429  0.711297  0.711297   \n",
       "83   0.571429        0.0          0.0    0.571429  0.711297  0.711297   \n",
       "\n",
       "    groupID  fullHistory  real  \n",
       "70        8           14   0.0  \n",
       "71        8           14   1.0  \n",
       "72        8           14   1.0  \n",
       "73        8           14   1.0  \n",
       "74        8           14   1.0  \n",
       "75        8           14   1.0  \n",
       "76        8           14   1.0  \n",
       "77        8           14   0.0  \n",
       "78        8           14   1.0  \n",
       "79        8           14   1.0  \n",
       "80        8           14   1.0  \n",
       "81        8           14   1.0  \n",
       "82        8           14   1.0  \n",
       "83        8           14   1.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b56e48-6500-44e2-b033-12c7ebca9c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40a52fe2-8e12-4923-a51b-f8ddd15bf108",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DOW:\n",
    "    PRE_FC, PRE_Seas, PRE_Traf, PRE_TF_timeseries = dow_get_tensors2(\n",
    "        Data_PRE,\n",
    "        sea_col_Cap,\n",
    "        prdMaps,\n",
    "        FC_time_series=FC_time_series,\n",
    "        traffic_time_series=traffic_time_series,\n",
    "        use_channels=use_channels,\n",
    "        seasenality_one_dimension=seasenality_one_dimension,\n",
    "        window=window,\n",
    "        random_masking=True,\n",
    "        test_today=None,\n",
    "    )\n",
    "    POST_FC, POST_Seas, POST_Traf, POST_TF_timeseries = dow_get_tensors2(\n",
    "        Data_POST,\n",
    "        sea_col_Cap,\n",
    "        prdMaps,\n",
    "        FC_time_series=FC_time_series,\n",
    "        traffic_time_series=traffic_time_series,\n",
    "        use_channels=use_channels,\n",
    "        seasenality_one_dimension=seasenality_one_dimension,\n",
    "        window=window,\n",
    "        random_masking=test_random_masking,\n",
    "        test_today=test_today,\n",
    "    )\n",
    "    # FUTURE_FC , FUTURE_Seas , FUTURE_Traf ,FUTUR_TF_timeseries = dow_get_tensors2(Data_FUTURE , sea_col_Cap, prdMaps  , FC_time_series = False , traffic_time_series = True ,  use_channels = True , seasenality_one_dimension = True ,   window = window)\n",
    "\n",
    "else:\n",
    "    PRE_FC, PRE_Seas, PRE_Traf, PRE_TF_timeseries = get_tensors2(\n",
    "        Data_PRE,\n",
    "        sea_col_Cap,\n",
    "        prdMaps,\n",
    "        FC_time_series=FC_time_series,\n",
    "        traffic_time_series=traffic_time_series,\n",
    "        use_channels=use_channels,\n",
    "        seasenality_one_dimension=seasenality_one_dimension,\n",
    "        window=window,\n",
    "    )\n",
    "\n",
    "    if test_random_masking:\n",
    "        POST_FC, POST_Seas, POST_Traf, POST_TF_timeseries = get_tensors2(\n",
    "            Data_POST,\n",
    "            sea_col_Cap,\n",
    "            prdMaps,\n",
    "            FC_time_series=FC_time_series,\n",
    "            traffic_time_series=traffic_time_series,\n",
    "            use_channels=use_channels,\n",
    "            seasenality_one_dimension=seasenality_one_dimension,\n",
    "            window=window,\n",
    "        )\n",
    "    else:\n",
    "        masked_df = create_masking_based_on_given_day(Data_POST, test_today, prdMaps)\n",
    "        POST_FC, POST_Seas, POST_Traf, POST_TF_timeseries = get_tensors2_faketoday(\n",
    "            Data_POST, masked_df, sea_col_Cap, use_channels, seasenality_one_dimension, window\n",
    "        )\n",
    "\n",
    "    # FUTURE_FC , FUTURE_Seas , FUTURE_Traf , FUTURE_TF_timeseries = get_tensors2(Data_FUTURE, sea_col_Cap, prdMaps , FC_time_series = False , traffic_time_series = True ,  use_channels = True , seasenality_one_dimension = True ,   window = window)\n",
    "\n",
    "# Train/Val Spilit:\n",
    "# TODO: THIS SHOULD BE CHANGED TO RANDOMIZED.\n",
    "train_val_cutoff = round(PRE_FC.shape[0] * train_val_percentage)\n",
    "\n",
    "# prepare train/val/test datasets\n",
    "PRE_FC_train = PRE_FC[:train_val_cutoff, :]\n",
    "PRE_FC_val = PRE_FC[train_val_cutoff:, :]\n",
    "\n",
    "PRE_Seas_train = PRE_Seas[:train_val_cutoff, :]\n",
    "PRE_Seas_val = PRE_Seas[train_val_cutoff:, :]\n",
    "\n",
    "PRE_Traf_train = PRE_Traf[:train_val_cutoff, :]\n",
    "PRE_Traf_val = PRE_Traf[train_val_cutoff:, :]\n",
    "\n",
    "PRE_TF_timeseries_train = PRE_TF_timeseries[:train_val_cutoff, :]\n",
    "PRE_TF_timeseries_val = PRE_TF_timeseries[train_val_cutoff:, :]\n",
    "\n",
    "train = [PRE_FC_train, PRE_Seas_train, PRE_TF_timeseries_train, PRE_Traf_train]\n",
    "val = [PRE_FC_val, PRE_Seas_val, PRE_TF_timeseries_val, PRE_Traf_val]\n",
    "test = [POST_FC, POST_Seas, POST_TF_timeseries, POST_Traf]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2780da91-e7a3-462d-a45a-49653f50fa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dow_get_tensors2(\n",
    "#     DataFarame,\n",
    "#     sea_col_Cap,\n",
    "#     prdMaps=None,\n",
    "#     FC_time_series=False,\n",
    "#     traffic_time_series=False,\n",
    "#     use_channels=False,\n",
    "#     seasenality_one_dimension=True,\n",
    "#     window=10,\n",
    "#     random_masking=True,\n",
    "#     test_today=None,\n",
    "# ):\n",
    "DOW = True\n",
    "DataFarame = Data_PRE\n",
    "random_masking = True\n",
    "FC_dow, Seasenality_dow, Traffic_dow, TF_time_dow = defaultdict(), defaultdict(), defaultdict(), defaultdict()\n",
    "\n",
    "if not random_masking:\n",
    "    masked_df = create_masking_based_on_given_day(DataFarame, test_today, prdMaps)\n",
    "\n",
    "for i in DataFarame.loc[:, [\"forecastDayOfWeek\"]].drop_duplicates().values:\n",
    "    # filter_y = DataFarame['dow_y' ] == i[1]\n",
    "    # filter_x = DataFarame['dow_x'] == i[0]\n",
    "    filter_dow = DataFarame[\"forecastDayOfWeek\"] == i[0]\n",
    "    # print(filter_y.shape , filter_x.shape)\n",
    "    # print(i)\n",
    "    Data_dow = DataFarame[filter_dow]\n",
    "    # print(Data_dow.shape)\n",
    "    if random_masking:\n",
    "        FC, Seasenality, Traffic, TF_time = get_tensors2(\n",
    "            Data_dow,\n",
    "            sea_col_Cap,\n",
    "            prdMaps,\n",
    "            FC_time_series,\n",
    "            traffic_time_series,\n",
    "            use_channels,\n",
    "            seasenality_one_dimension,\n",
    "            window,\n",
    "            DOW,\n",
    "        )\n",
    "    else:\n",
    "        Data_dow_masked = masked_df[filter_dow]\n",
    "        FC, Seasenality, Traffic, TF_time = get_tensors2_faketoday(\n",
    "            Data_dow, Data_dow_masked, sea_col_Cap, use_channels, seasenality_one_dimension, window\n",
    "        )\n",
    "\n",
    "    # FC, Seasenality, Traffic, TF_time= get_tensors2_faketoday(Data_dow, Data_dow_masked ,  sea_col_Cap , use_channels , seasenality_one_dimension ,  window)\n",
    "    for i, j in enumerate(Data_dow.index[::14][window:]):\n",
    "        FC_dow[j] = FC[i]\n",
    "        Seasenality_dow[j] = Seasenality[i]\n",
    "        Traffic_dow[j] = Traffic[i]\n",
    "        if traffic_time_series:\n",
    "            TF_time_dow[j] = TF_time[i]\n",
    "\n",
    "FC_dow = np.stack(list(dict(sorted(FC_dow.items())).values()))\n",
    "Seasenality_dow = np.stack(list(dict(sorted(Seasenality_dow.items())).values()))\n",
    "Traffic_dow = np.stack(list(dict(sorted(Traffic_dow.items())).values()))\n",
    "if traffic_time_series:\n",
    "    TF_time_dow = np.stack(list(dict(sorted(TF_time_dow.items())).values()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b97635d-207c-4384-aec1-8e684313d318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_tensors2(\n",
    "#     DataFarame,\n",
    "#     sea_col_Cap,\n",
    "#     prdMaps=None,\n",
    "#     FC_time_series=True,\n",
    "#     traffic_time_series=False,\n",
    "#     use_channels=False,\n",
    "#     seasenality_one_dimension=True,\n",
    "#     window=10,\n",
    "#     DOW=False,\n",
    "# ):\n",
    "DataFrame = Data_dow\n",
    "\n",
    "len_sea_cap = len(sea_col_Cap)\n",
    "\n",
    "# fractional closure\n",
    "PRE_FC_L = DataFarame[[\"fracClosure_\" + str(i + 1) for i in range(10)]].values.astype(\"float32\")\n",
    "# seasonality\n",
    "PRE_Sea_L = DataFarame[sea_col_Cap].values.astype(\"float32\")\n",
    "# actual traffic\n",
    "PRE_Traf_L = DataFarame[[\"trafficActual_\" + str(i + 1) for i in range(10)]].values.astype(\"float32\")\n",
    "\n",
    "# reshape the data for CNNLSTM model\n",
    "FC = PRE_FC_L.reshape(int(PRE_FC_L.shape[0] / 14), 1, 14, 10)\n",
    "Seasenality = PRE_Sea_L.reshape(int(PRE_Sea_L.shape[0] / 14), 1, 14, len_sea_cap)\n",
    "Traffic = PRE_Traf_L.reshape(int(PRE_Traf_L.shape[0] / 14), 1, 14, 10)\n",
    "\n",
    "# Remove Duplicates (from 2d to 1d vector)\n",
    "if seasenality_one_dimension:\n",
    "    Seasenality = np.delete(Seasenality, slice(13), 2).reshape(Seasenality.shape[0], len_sea_cap)\n",
    "\n",
    "if use_channels:\n",
    "    FC = FC.reshape(len(FC), 2, 7, 10)\n",
    "    Traffic = Traffic.reshape(len(Traffic), 2, 7, 10)\n",
    "\n",
    "# Change FC shape to refelect time series:\n",
    "# print(FC.shape)\n",
    "if FC_time_series:\n",
    "    time_series_widow = list()\n",
    "    Seasenality_times = list()\n",
    "    for i in range(window, len(FC)):\n",
    "        # print(FC[i-window:i].shape)\n",
    "        time_series_widow.append(FC[i - window : i].reshape(window, 2, 7, 10))\n",
    "        # print((Seasenality[i-window:i].shape))\n",
    "        Seasenality_times.append(Seasenality[i - window : i])\n",
    "    FC = np.array(time_series_widow)\n",
    "    Seasenality = np.array(Seasenality_times)\n",
    "\n",
    "    # Since the 1st window size data points are removed:\n",
    "    # Seasenality = Seasenality[window:]\n",
    "    Traffic = Traffic[window:]\n",
    "\n",
    "elif traffic_time_series:\n",
    "    traffic_time_series_window = list()\n",
    "    Seasenality_times = list()\n",
    "    for i in range(window, len(Traffic)):\n",
    "        # Find Random period and random day:\n",
    "        if DOW:\n",
    "            tf_window_masked = tf_timeseries_masking_DOW(Traffic, i, prdMaps, window)\n",
    "        else:\n",
    "            tf_window_masked = tf_timeseries_masking(Traffic, i, prdMaps, window)\n",
    "        traffic_time_series_window.append(tf_window_masked)\n",
    "        # Seasenality_times.append(Seasenality[i-window:i])\n",
    "    TF_time = np.array(traffic_time_series_window)\n",
    "    # Seasenality = np.array(Seasenality_times)\n",
    "    Seasenality = Seasenality[window:]\n",
    "    FC = FC[window:]\n",
    "\n",
    "    Traffic = Traffic[window:]\n",
    "\n",
    "    # return FC, Seasenality, Traffic, TF_time\n",
    "\n",
    "# return FC, Seasenality, Traffic, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b3530e47-c59b-470f-891c-71783ebda870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "# def tf_timeseries_masking_DOW(tf_tensors, data_index, prdMaps, window):\n",
    "    # \"\"\"This function will generate masked time-seried terrafic data, and is based on DOW.\"\"\"\n",
    "tf_tensors = Traffic\n",
    "data_index = 15\n",
    "    \n",
    "random_period, random_day_to_dept = randPeriod(prdMaps)\n",
    "arr = prdMaps.iloc[:, 3].values\n",
    "test_tensors = tf_tensors.copy()\n",
    "\n",
    "day_to_dept = random_day_to_dept\n",
    "current_index = data_index\n",
    "print(day_to_dept)\n",
    "for i in range(0, window):\n",
    "    # Move back 7 days in each iter.\n",
    "    day_to_dept = random_day_to_dept - i * 7\n",
    "    # Get the period of that day to dept.\n",
    "    flrs = floorSearch(arr, 0, 6, day_to_dept)\n",
    "    current_period = flrs + 1\n",
    "    # If we get today, will break the loop. and use all the values (no masking)\n",
    "    if current_period == 0:\n",
    "        break\n",
    "    # mask the values\n",
    "    test_tensors[\n",
    "        current_index,\n",
    "        :,\n",
    "        :current_period,\n",
    "    ] = -1\n",
    "\n",
    "    # Update index:\n",
    "    current_index -= 1\n",
    "\n",
    "    # return test_tensors[data_index + 1 - window : data_index + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cfa28cb1-9d44-45e9-9c62-f23ddaeb9dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.,  0.,  0., ...,  2.,  1.,  0.],\n",
       "         [ 0.,  0.,  1., ...,  2.,  4.,  7.],\n",
       "         [ 0.,  0.,  0., ...,  6.,  0.,  5.],\n",
       "         ...,\n",
       "         [ 1.,  0.,  0., ...,  0.,  8.,  3.],\n",
       "         [ 0.,  0.,  0., ...,  0., 10.,  3.],\n",
       "         [ 0.,  6.,  0., ...,  0.,  1.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  5., ...,  3.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  4.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "       [[[ 0.,  0.,  0., ...,  0.,  4.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "         [ 2.,  0.,  0., ...,  0.,  3.,  2.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  1., ...,  0.,  7.,  4.],\n",
       "         [ 0.,  0.,  0., ...,  0., 11.,  6.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0., 10., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ...,  0.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  1., ...,  0.,  1.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "       [[[ 0.,  0.,  0., ...,  1.,  2.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  2.,  0.,  8.],\n",
       "         [ 0.,  0.,  0., ...,  1.,  4.,  5.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  9., 19.],\n",
       "         [ 0.,  0.,  0., ...,  2., 18.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  2., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  8.,  1.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[-1., -1., -1., ..., -1., -1., -1.],\n",
       "         [-1., -1., -1., ..., -1., -1., -1.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  4.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  4.,  4.],\n",
       "         [ 0.,  1.,  2., ...,  1.,  4.,  4.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  4.,  0.]],\n",
       "\n",
       "        [[-1., -1., -1., ..., -1., -1., -1.],\n",
       "         [-1., -1., -1., ..., -1., -1., -1.],\n",
       "         [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  3.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  2.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "       [[[-1., -1., -1., ..., -1., -1., -1.],\n",
       "         [-1., -1., -1., ..., -1., -1., -1.],\n",
       "         [-1., -1., -1., ..., -1., -1., -1.],\n",
       "         ...,\n",
       "         [ 0.,  1.,  0., ...,  0.,  3.,  8.],\n",
       "         [ 0.,  0.,  1., ...,  0., 16.,  7.],\n",
       "         [ 1.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[-1., -1., -1., ..., -1., -1., -1.],\n",
       "         [-1., -1., -1., ..., -1., -1., -1.],\n",
       "         [-1., -1., -1., ..., -1., -1., -1.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  5.,  1.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  2.,  1.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "       [[[-1., -1., -1., ..., -1., -1., -1.],\n",
       "         [-1., -1., -1., ..., -1., -1., -1.],\n",
       "         [-1., -1., -1., ..., -1., -1., -1.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ..., 11.,  4.,  6.],\n",
       "         [ 1.,  0.,  0., ...,  2., 25., 12.],\n",
       "         [ 0.,  0.,  1., ...,  0.,  1.,  0.]],\n",
       "\n",
       "        [[-1., -1., -1., ..., -1., -1., -1.],\n",
       "         [-1., -1., -1., ..., -1., -1., -1.],\n",
       "         [-1., -1., -1., ..., -1., -1., -1.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  2.,  0.,  0.]]]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tensors[data_index + 1 - window : data_index + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb3e7291-ffef-4c78-a20d-372d4f647de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "floorSearch(arr, 0, 6, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18c714d6-2b3c-4275-a1e7-3cb6f9ee8722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>DESTINATION</th>\n",
       "      <th>FORECASTPERIOD</th>\n",
       "      <th>RRD_START</th>\n",
       "      <th>RRD_END</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DFW</td>\n",
       "      <td>TUS</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DFW</td>\n",
       "      <td>TUS</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DFW</td>\n",
       "      <td>TUS</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DFW</td>\n",
       "      <td>TUS</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DFW</td>\n",
       "      <td>TUS</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DFW</td>\n",
       "      <td>TUS</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DFW</td>\n",
       "      <td>TUS</td>\n",
       "      <td>7</td>\n",
       "      <td>150</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ORIGIN DESTINATION  FORECASTPERIOD  RRD_START  RRD_END\n",
       "0    DFW         TUS               1          2        6\n",
       "1    DFW         TUS               2          7       13\n",
       "2    DFW         TUS               3         14       20\n",
       "3    DFW         TUS               4         21       29\n",
       "4    DFW         TUS               5         30       49\n",
       "5    DFW         TUS               6         50      149\n",
       "6    DFW         TUS               7        150      331"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prdMaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "528e5024-a80e-4fba-94f8-24fc0e0f9efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 40)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randPeriod(prdMaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d7eb93f-3b9a-49aa-b4aa-ba10d369058b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  1.,  2.,  2.],\n",
       "         [ 0.,  0.,  0., ...,  2.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  1.,  0., ...,  0.,  5.,  5.],\n",
       "         [ 0.,  0.,  1., ...,  0.,  1.,  7.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  6., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  1., ...,  3.,  0.,  1.],\n",
       "         [ 0.,  0.,  0., ...,  2.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "       [[[ 0.,  0.,  0., ...,  1.,  2.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  6.,  3.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  4.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  3., 17.,  8.],\n",
       "         [ 2.,  0.,  0., ...,  0., 26.,  5.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  1.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  1., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0.,  1., ...,  2.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  1., ...,  3.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  2.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "       [[[ 0.,  0.,  0., ...,  3.,  6.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  2.,  2.,  8.],\n",
       "         [ 2.,  0.,  0., ...,  0.,  7., 10.],\n",
       "         ...,\n",
       "         [ 1.,  0.,  0., ...,  2., 14.,  8.],\n",
       "         [ 0.,  0.,  0., ...,  1., 10.,  3.],\n",
       "         [ 0.,  0.,  0., ...,  2.,  0.,  2.]],\n",
       "\n",
       "        [[ 0.,  0.,  3., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  5.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  3.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 0.,  0.,  0., ...,  1.,  3.,  0.],\n",
       "         [ 0.,  1.,  0., ...,  1.,  7.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  1.,  5.,  1.],\n",
       "         ...,\n",
       "         [ 1.,  0.,  0., ...,  0.,  1.,  0.],\n",
       "         [ 1.,  1.,  0., ...,  0.,  0.,  2.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 1.,  0.,  0., ...,  0.,  2.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  1.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "       [[[ 2.,  1.,  1., ...,  4.,  7.,  0.],\n",
       "         [ 0.,  0.,  2., ...,  2.,  5.,  1.],\n",
       "         [ 0.,  0.,  0., ...,  3., 16.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  1.,  4.,  6.],\n",
       "         [ 0.,  0.,  0., ...,  2., 12.,  5.],\n",
       "         [ 0.,  1.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  7., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  1.,  0., ...,  1.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  0.,  3.,  2.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  2.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "       [[[ 0.,  0.,  1., ...,  3.,  0.,  0.],\n",
       "         [ 1.,  0.,  0., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0.,  1., ...,  2.,  7.,  0.],\n",
       "         ...,\n",
       "         [ 1.,  0.,  0., ...,  2., 16.,  0.],\n",
       "         [ 1.,  0.,  0., ...,  6.,  8.,  4.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  2.,  0.]],\n",
       "\n",
       "        [[ 0.,  6.,  3., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  1.,  0., ...,  0.,  2.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  3.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ...,  3.,  4.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  1., 11.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]]]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85ca1a1c-4f51-4dd1-83c1-e063a6d9110e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(855, 2, 7, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def randPeriod(prdMaps):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        prdMaps (Dataframe): Dataframe that shows the time to departure where the period class of a given flight gets closed.\n",
    "\n",
    "    Returns:\n",
    "        random_period (int): Random Class to departure. (between 1-7)\n",
    "        random_day (int): Random Day to Departure (it should be between 2 to 331- When the fist class opens up.)\n",
    "    \"\"\"\n",
    "    random_period = np.random.randint(1, 7)  # Gets a class between 1 to 7, these are our period to departure classes.\n",
    "    rrd_start, rrt_end = prdMaps[prdMaps[\"FORECASTPERIOD\"] == random_period].loc[:, [\"RRD_START\", \"RRD_END\"]].values[0]\n",
    "    random_day = np.random.randint(rrd_start, rrt_end)\n",
    "    return random_period, random_day\n",
    "\n",
    "\n",
    "def tf_timeseries_masking(tf_tensors, data_index, prdMaps, window):\n",
    "    \"\"\"This function will generate masked time-seried terrafic data.\"\"\"\n",
    "\n",
    "    random_period, random_day_to_dept = randPeriod(prdMaps)\n",
    "    # print(random_period , random_day_to_dept )\n",
    "    arr = prdMaps.iloc[:, 3].values\n",
    "\n",
    "    # output = tf_tensors[data_index].copy()\n",
    "    test_tensors = tf_tensors.copy()\n",
    "    test_tensors[data_index][\n",
    "        :,\n",
    "        :random_period,\n",
    "    ] = -1\n",
    "\n",
    "    max_bond_period = random_day_to_dept\n",
    "    min_bond_period = arr[random_period - 1]\n",
    "    remaining_window = window - 1\n",
    "    current_index = data_index\n",
    "    max_min_range = max_bond_period - min_bond_period\n",
    "    current_period = random_period\n",
    "\n",
    "    if max_min_range < remaining_window:\n",
    "        while max_min_range <= remaining_window:\n",
    "            # print(current_index-max_min_range,current_index)\n",
    "            test_tensors[\n",
    "                current_index - max_min_range : current_index,\n",
    "                :,\n",
    "                :current_period,\n",
    "            ] = -1\n",
    "            current_period -= 1\n",
    "            if current_period == 0:\n",
    "                break\n",
    "            current_index -= max_min_range\n",
    "            remaining_window -= max_min_range\n",
    "            max_bond_period -= max_min_range\n",
    "            min_bond_period = arr[current_period - 1]\n",
    "            max_min_range = max_bond_period - min_bond_period\n",
    "            # reaching Today date:\n",
    "\n",
    "    if max_min_range >= remaining_window:\n",
    "        # print(current_index-max_min_range,current_index)\n",
    "        test_tensors[\n",
    "            current_index - remaining_window : current_index,\n",
    "            :,\n",
    "            :current_period,\n",
    "        ] = -1\n",
    "\n",
    "    return test_tensors[data_index + 1 - window : data_index + 1]\n",
    "\n",
    "\n",
    "def get_tensors2(\n",
    "    DataFarame,\n",
    "    sea_col_Cap,\n",
    "    prdMaps=None,\n",
    "    FC_time_series=True,\n",
    "    traffic_time_series=False,\n",
    "    use_channels=False,\n",
    "    seasenality_one_dimension=True,\n",
    "    window=10,\n",
    "    DOW=False,\n",
    "):\n",
    "\n",
    "    len_sea_cap = len(sea_col_Cap)\n",
    "\n",
    "    # fractional closure\n",
    "    PRE_FC_L = DataFarame[[\"fracClosure_\" + str(i + 1) for i in range(10)]].values.astype(\"float32\")\n",
    "    # seasonality\n",
    "    PRE_Sea_L = DataFarame[sea_col_Cap].values.astype(\"float32\")\n",
    "    # actual traffic\n",
    "    PRE_Traf_L = DataFarame[[\"trafficActual_\" + str(i + 1) for i in range(10)]].values.astype(\"float32\")\n",
    "\n",
    "    # reshape the data for CNNLSTM model\n",
    "    FC = PRE_FC_L.reshape(int(PRE_FC_L.shape[0] / 14), 1, 14, 10)\n",
    "    Seasenality = PRE_Sea_L.reshape(int(PRE_Sea_L.shape[0] / 14), 1, 14, len_sea_cap)\n",
    "    Traffic = PRE_Traf_L.reshape(int(PRE_Traf_L.shape[0] / 14), 1, 14, 10)\n",
    "\n",
    "    # Remove Duplicates (from 2d to 1d vector)\n",
    "    if seasenality_one_dimension:\n",
    "        Seasenality = np.delete(Seasenality, slice(13), 2).reshape(Seasenality.shape[0], len_sea_cap)\n",
    "\n",
    "    if use_channels:\n",
    "        FC = FC.reshape(len(FC), 2, 7, 10)\n",
    "        Traffic = Traffic.reshape(len(Traffic), 2, 7, 10)\n",
    "\n",
    "    # Change FC shape to refelect time series:\n",
    "    # print(FC.shape)\n",
    "    if FC_time_series:\n",
    "        time_series_widow = list()\n",
    "        Seasenality_times = list()\n",
    "        for i in range(window, len(FC)):\n",
    "            # print(FC[i-window:i].shape)\n",
    "            time_series_widow.append(FC[i - window : i].reshape(window, 2, 7, 10))\n",
    "            # print((Seasenality[i-window:i].shape))\n",
    "            Seasenality_times.append(Seasenality[i - window : i])\n",
    "        FC = np.array(time_series_widow)\n",
    "        Seasenality = np.array(Seasenality_times)\n",
    "\n",
    "        # Since the 1st window size data points are removed:\n",
    "        # Seasenality = Seasenality[window:]\n",
    "        Traffic = Traffic[window:]\n",
    "\n",
    "    elif traffic_time_series:\n",
    "        traffic_time_series_window = list()\n",
    "        Seasenality_times = list()\n",
    "        for i in range(window, len(Traffic)):\n",
    "            # Find Random period and random day:\n",
    "            if DOW:\n",
    "                tf_window_masked = tf_timeseries_masking_DOW(Traffic, i, prdMaps, window)\n",
    "            else:\n",
    "                tf_window_masked = tf_timeseries_masking(Traffic, i, prdMaps, window)\n",
    "            traffic_time_series_window.append(tf_window_masked)\n",
    "            # Seasenality_times.append(Seasenality[i-window:i])\n",
    "        TF_time = np.array(traffic_time_series_window)\n",
    "        # Seasenality = np.array(Seasenality_times)\n",
    "        Seasenality = Seasenality[window:]\n",
    "        FC = FC[window:]\n",
    "\n",
    "        Traffic = Traffic[window:]\n",
    "\n",
    "        return FC, Seasenality, Traffic, TF_time\n",
    "\n",
    "    return FC, Seasenality, Traffic, None\n",
    "\n",
    "\n",
    "def floorSearch(arr, low, high, x):\n",
    "\n",
    "    # If low and high cross each other\n",
    "    if low > high:\n",
    "        return -1\n",
    "\n",
    "    # If last element is smaller than x\n",
    "    if x >= arr[high]:\n",
    "        return high\n",
    "\n",
    "    # Find the middle point\n",
    "    mid = int((low + high) / 2)\n",
    "\n",
    "    # If middle point is floor.\n",
    "    if arr[mid] == x:\n",
    "        return mid\n",
    "\n",
    "    # If x lies between mid-1 and mid\n",
    "    if mid > 0 and arr[mid - 1] <= x and x < arr[mid]:\n",
    "        return mid - 1\n",
    "\n",
    "    # If x is smaller than mid,\n",
    "    # floor must be in left half.\n",
    "    if x < arr[mid]:\n",
    "        return floorSearch(arr, low, mid - 1, x)\n",
    "\n",
    "    # If mid-1 is not floor and x is greater than\n",
    "    # arr[mid],\n",
    "    return floorSearch(arr, mid + 1, high, x)\n",
    "\n",
    "\n",
    "def tf_timeseries_masking_DOW(tf_tensors, data_index, prdMaps, window):\n",
    "    \"\"\"This function will generate masked time-seried terrafic data, and is based on DOW.\"\"\"\n",
    "\n",
    "    random_period, random_day_to_dept = randPeriod(prdMaps)\n",
    "    arr = prdMaps.iloc[:, 3].values\n",
    "    test_tensors = tf_tensors.copy()\n",
    "\n",
    "    day_to_dept = random_day_to_dept\n",
    "    current_index = data_index\n",
    "\n",
    "    for i in range(0, window):\n",
    "        # Move back 7 days in each iter.\n",
    "        day_to_dept = random_day_to_dept - i * 7\n",
    "        # Get the period of that day to dept.\n",
    "        flrs = floorSearch(arr, 0, 6, day_to_dept)\n",
    "        current_period = flrs + 1\n",
    "        # If we get today, will break the loop. and use all the values (no masking)\n",
    "        if current_period == 0:\n",
    "            break\n",
    "        # mask the values\n",
    "        test_tensors[\n",
    "            current_index,\n",
    "            :,\n",
    "            :current_period,\n",
    "        ] = -1\n",
    "\n",
    "        # Update index:\n",
    "        current_index -= 1\n",
    "\n",
    "    return test_tensors[data_index + 1 - window : data_index + 1]\n",
    "\n",
    "\n",
    "def get_prdMaps(orig, dest, hcrt):\n",
    "    \"\"\"TODO: add the lcl_flw_ind and change the data to mask diffrent between the local and Flow Traffics\"\"\"\n",
    "\n",
    "    prdMaps = pd.read_sql(\n",
    "        f\"\"\"select DISTINCT leg_orig as origin, leg_dest as destination, fcst_period as forecastPeriod, rrd_band_start_i as rrd_start, rrd_band_end_i as rrd_end\n",
    "                            -- , lcl_flw_ind\n",
    "                            from market_xref a\n",
    "                            join FCST.FCST_PERIOD_REF b\n",
    "                            on a.infl_period_id = b.FCST_PERIOD_ID\n",
    "                            where 1=1\n",
    "                            and cabin_code = 'Y'\n",
    "                            and leg_orig = '{orig}'\n",
    "                            and leg_dest = '{dest}'\n",
    "                            and lcl_flw_ind = 'L'\n",
    "                            ORDER BY forecastPeriod\n",
    "                            \"\"\",\n",
    "        con=hcrt,\n",
    "    )\n",
    "    return prdMaps\n",
    "\n",
    "\n",
    "# def dow_get_tensors2(DataFarame , sea_col_Cap, prdMaps= None  ,  test = False, time_series = True,  use_channels = False , window = 10):\n",
    "# def dow_get_tensors2(DataFarame , sea_col_Cap, prdMaps= None  ,  FC_time_series = True , traffic_time_series = False ,  use_channels = False , seasenality_one_dimension = True ,  window = 10):\n",
    "#     DOW = True\n",
    "#     FC_dow , Seasenality_dow, Traffic_dow ,  TF_time_dow  = list(), list(), list(), list()\n",
    "\n",
    "#     for i in DataFarame.loc[ :,\t['forecastDayOfWeek' ]].drop_duplicates().values:\n",
    "#         # filter_y = DataFarame['dow_y' ] == i[1]\n",
    "#         # filter_x = DataFarame['dow_x'] == i[0]\n",
    "#         filter_dow =  DataFarame['forecastDayOfWeek'] == i[0]\n",
    "#         # print(filter_y.shape , filter_x.shape)\n",
    "#         # print(i)\n",
    "#         Data_dow =DataFarame[filter_dow]\n",
    "#         # print(Data_dow.shape)\n",
    "#         FC, Seasenality, Traffic, TF_time= get_tensors2(Data_dow, sea_col_Cap, prdMaps  , FC_time_series  , traffic_time_series ,  use_channels  , seasenality_one_dimension  ,  window, DOW )\n",
    "#         FC_dow.append(FC)\n",
    "#         Seasenality_dow.append(Seasenality)\n",
    "#         Traffic_dow.append(Traffic)\n",
    "#         if traffic_time_series:\n",
    "#             TF_time_dow.append(TF_time)\n",
    "\n",
    "#     # Then Concat together, now each datapoint is based on DOW.\n",
    "#     FC_dow = [ i  for i in FC_dow if i.shape!=(0,)]\n",
    "#     Seasenality_dow = [ i  for i in Seasenality_dow if i.shape!=(0,)]\n",
    "#     # Traffic_dow = [ i  for i in Traffic_dow if i.shape!=(0,)]\n",
    "\n",
    "#     if traffic_time_series:\n",
    "#         TF_time_dow = [ i  for i in TF_time_dow if i.shape!=(0,)]\n",
    "#         TF_time_dow = np.concatenate(TF_time_dow)\n",
    "\n",
    "#     FC_dow = np.concatenate(FC_dow)\n",
    "#     Seasenality_dow = np.concatenate(Seasenality_dow)\n",
    "#     Traffic_dow = np.concatenate(Traffic_dow)\n",
    "\n",
    "#     return FC_dow , Seasenality_dow, Traffic_dow , TF_time_dow\n",
    "\n",
    "\n",
    "def dow_get_tensors2(\n",
    "    DataFarame,\n",
    "    sea_col_Cap,\n",
    "    prdMaps=None,\n",
    "    FC_time_series=False,\n",
    "    traffic_time_series=False,\n",
    "    use_channels=False,\n",
    "    seasenality_one_dimension=True,\n",
    "    window=10,\n",
    "    random_masking=True,\n",
    "    test_today=None,\n",
    "):\n",
    "    DOW = True\n",
    "    FC_dow, Seasenality_dow, Traffic_dow, TF_time_dow = defaultdict(), defaultdict(), defaultdict(), defaultdict()\n",
    "\n",
    "    if not random_masking:\n",
    "        masked_df = create_masking_based_on_given_day(DataFarame, test_today, prdMaps)\n",
    "\n",
    "    for i in DataFarame.loc[:, [\"forecastDayOfWeek\"]].drop_duplicates().values:\n",
    "        # filter_y = DataFarame['dow_y' ] == i[1]\n",
    "        # filter_x = DataFarame['dow_x'] == i[0]\n",
    "        filter_dow = DataFarame[\"forecastDayOfWeek\"] == i[0]\n",
    "        # print(filter_y.shape , filter_x.shape)\n",
    "        # print(i)\n",
    "        Data_dow = DataFarame[filter_dow]\n",
    "        # print(Data_dow.shape)\n",
    "        if random_masking:\n",
    "            FC, Seasenality, Traffic, TF_time = get_tensors2(\n",
    "                Data_dow,\n",
    "                sea_col_Cap,\n",
    "                prdMaps,\n",
    "                FC_time_series,\n",
    "                traffic_time_series,\n",
    "                use_channels,\n",
    "                seasenality_one_dimension,\n",
    "                window,\n",
    "                DOW,\n",
    "            )\n",
    "        else:\n",
    "            Data_dow_masked = masked_df[filter_dow]\n",
    "            FC, Seasenality, Traffic, TF_time = get_tensors2_faketoday(\n",
    "                Data_dow, Data_dow_masked, sea_col_Cap, use_channels, seasenality_one_dimension, window\n",
    "            )\n",
    "\n",
    "        # FC, Seasenality, Traffic, TF_time= get_tensors2_faketoday(Data_dow, Data_dow_masked ,  sea_col_Cap , use_channels , seasenality_one_dimension ,  window)\n",
    "        for i, j in enumerate(Data_dow.index[::14][window:]):\n",
    "            FC_dow[j] = FC[i]\n",
    "            Seasenality_dow[j] = Seasenality[i]\n",
    "            Traffic_dow[j] = Traffic[i]\n",
    "            if traffic_time_series:\n",
    "                TF_time_dow[j] = TF_time[i]\n",
    "\n",
    "    FC_dow = np.stack(list(dict(sorted(FC_dow.items())).values()))\n",
    "    Seasenality_dow = np.stack(list(dict(sorted(Seasenality_dow.items())).values()))\n",
    "    Traffic_dow = np.stack(list(dict(sorted(Traffic_dow.items())).values()))\n",
    "    if traffic_time_series:\n",
    "        TF_time_dow = np.stack(list(dict(sorted(TF_time_dow.items())).values()))\n",
    "\n",
    "    return FC_dow, Seasenality_dow, Traffic_dow, TF_time_dow\n",
    "\n",
    "\n",
    "def create_masking_based_on_given_day(DataFrame, test_today, prdMaps):\n",
    "    \"\"\"This function maskes the data absed on a given date.\"\"\"\n",
    "    arr = prdMaps.iloc[:, 3].values\n",
    "    try:\n",
    "        test_today_index = int(DataFrame[DataFrame[\"forecastDepartureDate\"] >= test_today].index[0] / 14)\n",
    "    except Exception:\n",
    "        print(\"No Date after the set fake today date\")\n",
    "        return DataFrame\n",
    "    test_df = DataFrame.copy()\n",
    "\n",
    "    current_period = 1\n",
    "    day_from_today = 0\n",
    "    for current_index in range(test_today_index, len(test_df) // 14):\n",
    "\n",
    "        if current_period < 7:\n",
    "            if day_from_today == arr[current_period]:\n",
    "                current_period += 1\n",
    "                # print(current_period)\n",
    "        else:\n",
    "            current_period = 7\n",
    "\n",
    "        day_data_df = test_df[current_index * 14 : (current_index + 1) * 14]\n",
    "        day_data_df.loc[day_data_df[\"forecastPeriod\"] <= current_period, \"trafficActual_1\":\"trafficActualAadv_10\"] = -1\n",
    "\n",
    "        day_from_today = day_from_today + 1\n",
    "        # break\n",
    "    return test_df\n",
    "\n",
    "\n",
    "def get_tensors2_faketoday(\n",
    "    DataFarame, DataFarame_Masked, sea_col_Cap, use_channels=False, seasenality_one_dimension=True, window=10\n",
    "):\n",
    "    \"\"\"This function uses a masekd dataframe. (it is used when we want to set a fake_today for our test set\"\"\"\n",
    "\n",
    "    len_sea_cap = len(sea_col_Cap)\n",
    "\n",
    "    # fractional closure\n",
    "    PRE_FC_L = DataFarame[[\"fracClosure_\" + str(i + 1) for i in range(10)]].values.astype(\"float32\")\n",
    "    # seasonality\n",
    "    PRE_Sea_L = DataFarame[sea_col_Cap].values.astype(\"float32\")\n",
    "    # actual traffic\n",
    "    PRE_Traf_L = DataFarame[[\"trafficActual_\" + str(i + 1) for i in range(10)]].values.astype(\"float32\")\n",
    "    # Masked Traffic\n",
    "    PRE_Traf_L_Masked = DataFarame_Masked[[\"trafficActual_\" + str(i + 1) for i in range(10)]].values.astype(\"float32\")\n",
    "\n",
    "    # reshape the data for CNNLSTM model\n",
    "    FC = PRE_FC_L.reshape(int(PRE_FC_L.shape[0] / 14), 1, 14, 10)\n",
    "    Seasenality = PRE_Sea_L.reshape(int(PRE_Sea_L.shape[0] / 14), 1, 14, len_sea_cap)\n",
    "    Traffic = PRE_Traf_L.reshape(int(PRE_Traf_L.shape[0] / 14), 1, 14, 10)\n",
    "    Traffic_Masked = PRE_Traf_L_Masked.reshape(int(PRE_Traf_L_Masked.shape[0] / 14), 1, 14, 10)\n",
    "\n",
    "    # Remove Duplicates (from 2d to 1d vector)\n",
    "    if seasenality_one_dimension:\n",
    "        Seasenality = np.delete(Seasenality, slice(13), 2).reshape(Seasenality.shape[0], len_sea_cap)\n",
    "\n",
    "    if use_channels:\n",
    "        FC = FC.reshape(len(FC), 2, 7, 10)\n",
    "        Traffic = Traffic.reshape(len(Traffic), 2, 7, 10)\n",
    "        Traffic_Masked = Traffic_Masked.reshape(len(Traffic_Masked), 2, 7, 10)\n",
    "\n",
    "    traffic_time_series_window = list()\n",
    "    # Seasenality_times = list()\n",
    "    for i in range(window, len(Traffic)):\n",
    "        # Get Masked Matrix\n",
    "        tf_window_masked = Traffic_Masked[i + 1 - window : i + 1]\n",
    "        traffic_time_series_window.append(tf_window_masked)\n",
    "        # Seasenality_times.append(Seasenality[i-window:i])\n",
    "    TF_time = np.array(traffic_time_series_window)\n",
    "    # Seasenality = np.array(Seasenality_times)\n",
    "    Seasenality = Seasenality[window:]\n",
    "    FC = FC[window:]\n",
    "\n",
    "    Traffic = Traffic[window:]\n",
    "\n",
    "    return FC, Seasenality, Traffic, TF_time\n",
    "\n",
    "\n",
    "def get_train_test_samples2(\n",
    "    Data_PRE,\n",
    "    Data_POST,\n",
    "    Data_FUTURE,\n",
    "    sea_col_Cap,\n",
    "    prdMaps,\n",
    "    DOW=False,\n",
    "    train_val_percentage=0.9,\n",
    "    FC_time_series=True,\n",
    "    traffic_time_series=False,\n",
    "    use_channels=False,\n",
    "    seasenality_one_dimension=True,\n",
    "    window=10,\n",
    "    test_random_masking=True,\n",
    "    test_today=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    test_today format =  yyyy-mm-dd\n",
    "    \"\"\"\n",
    "\n",
    "    if DOW:\n",
    "        PRE_FC, PRE_Seas, PRE_Traf, PRE_TF_timeseries = dow_get_tensors2(\n",
    "            Data_PRE,\n",
    "            sea_col_Cap,\n",
    "            prdMaps,\n",
    "            FC_time_series=FC_time_series,\n",
    "            traffic_time_series=traffic_time_series,\n",
    "            use_channels=use_channels,\n",
    "            seasenality_one_dimension=seasenality_one_dimension,\n",
    "            window=window,\n",
    "            random_masking=True,\n",
    "            test_today=None,\n",
    "        )\n",
    "        POST_FC, POST_Seas, POST_Traf, POST_TF_timeseries = dow_get_tensors2(\n",
    "            Data_POST,\n",
    "            sea_col_Cap,\n",
    "            prdMaps,\n",
    "            FC_time_series=FC_time_series,\n",
    "            traffic_time_series=traffic_time_series,\n",
    "            use_channels=use_channels,\n",
    "            seasenality_one_dimension=seasenality_one_dimension,\n",
    "            window=window,\n",
    "            random_masking=test_random_masking,\n",
    "            test_today=test_today,\n",
    "        )\n",
    "        # FUTURE_FC , FUTURE_Seas , FUTURE_Traf ,FUTUR_TF_timeseries = dow_get_tensors2(Data_FUTURE , sea_col_Cap, prdMaps  , FC_time_series = False , traffic_time_series = True ,  use_channels = True , seasenality_one_dimension = True ,   window = window)\n",
    "\n",
    "    else:\n",
    "        PRE_FC, PRE_Seas, PRE_Traf, PRE_TF_timeseries = get_tensors2(\n",
    "            Data_PRE,\n",
    "            sea_col_Cap,\n",
    "            prdMaps,\n",
    "            FC_time_series=FC_time_series,\n",
    "            traffic_time_series=traffic_time_series,\n",
    "            use_channels=use_channels,\n",
    "            seasenality_one_dimension=seasenality_one_dimension,\n",
    "            window=window,\n",
    "        )\n",
    "\n",
    "        if test_random_masking:\n",
    "            POST_FC, POST_Seas, POST_Traf, POST_TF_timeseries = get_tensors2(\n",
    "                Data_POST,\n",
    "                sea_col_Cap,\n",
    "                prdMaps,\n",
    "                FC_time_series=FC_time_series,\n",
    "                traffic_time_series=traffic_time_series,\n",
    "                use_channels=use_channels,\n",
    "                seasenality_one_dimension=seasenality_one_dimension,\n",
    "                window=window,\n",
    "            )\n",
    "        else:\n",
    "            masked_df = create_masking_based_on_given_day(Data_POST, test_today, prdMaps)\n",
    "            POST_FC, POST_Seas, POST_Traf, POST_TF_timeseries = get_tensors2_faketoday(\n",
    "                Data_POST, masked_df, sea_col_Cap, use_channels, seasenality_one_dimension, window\n",
    "            )\n",
    "\n",
    "        # FUTURE_FC , FUTURE_Seas , FUTURE_Traf , FUTURE_TF_timeseries = get_tensors2(Data_FUTURE, sea_col_Cap, prdMaps , FC_time_series = False , traffic_time_series = True ,  use_channels = True , seasenality_one_dimension = True ,   window = window)\n",
    "\n",
    "    # Train/Val Spilit:\n",
    "    # TODO: THIS SHOULD BE CHANGED TO RANDOMIZED.\n",
    "    train_val_cutoff = round(PRE_FC.shape[0] * train_val_percentage)\n",
    "\n",
    "    # prepare train/val/test datasets\n",
    "    PRE_FC_train = PRE_FC[:train_val_cutoff, :]\n",
    "    PRE_FC_val = PRE_FC[train_val_cutoff:, :]\n",
    "\n",
    "    PRE_Seas_train = PRE_Seas[:train_val_cutoff, :]\n",
    "    PRE_Seas_val = PRE_Seas[train_val_cutoff:, :]\n",
    "\n",
    "    PRE_Traf_train = PRE_Traf[:train_val_cutoff, :]\n",
    "    PRE_Traf_val = PRE_Traf[train_val_cutoff:, :]\n",
    "\n",
    "    PRE_TF_timeseries_train = PRE_TF_timeseries[:train_val_cutoff, :]\n",
    "    PRE_TF_timeseries_val = PRE_TF_timeseries[train_val_cutoff:, :]\n",
    "\n",
    "    train = [PRE_FC_train, PRE_Seas_train, PRE_TF_timeseries_train, PRE_Traf_train]\n",
    "    val = [PRE_FC_val, PRE_Seas_val, PRE_TF_timeseries_val, PRE_Traf_val]\n",
    "    test = [POST_FC, POST_Seas, POST_TF_timeseries, POST_Traf]\n",
    "\n",
    "    return train, val, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad580bd-c38e-4ea4-abb2-b2a92c12ae88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1976d260-7244-41cd-a327-2ed46b37edcc",
   "metadata": {},
   "source": [
    "### Training Loop:\n",
    "\n",
    "Run the model given one Hub:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece3dc5c-7cb8-4e7c-845d-f1e16ac9ca15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "orig = \"DFW\" #HUB\n",
    "switch_orig_dist = False # When you want to Check flights to your HUb (VS. From your hub)\n",
    "\n",
    "DOW= True\n",
    "\n",
    "# If test_random_masking is False -> Means you have to give it a date for creating a fake today\n",
    "test_random_masking = False\n",
    "test_today = '2022-06-01'\n",
    "\n",
    "yesterday =  datetime.today() - timedelta(days=2)\n",
    "next_year_today = datetime.today() + timedelta(days=365)\n",
    "\n",
    "pull_start = '2017-09-01'\n",
    "pull_end = next_year_today.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Pre: pre-covid period, used for train and validation\n",
    "Pre_start, Pre_end = '2017-09-01', '2020-01-30'\n",
    "# Post: post-covid period, used for test\n",
    "Post_start, Post_end = '2021-07-01',  yesterday.strftime(\"%Y-%m-%d\")\n",
    "# Train on All:\n",
    "\n",
    "# Future: Today till one year in future:\n",
    "future_start , future_end = datetime.today().strftime(\"%Y-%m-%d\") ,   next_year_today.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "new_market = False # change this to True if it is a new market\n",
    "ulcc_list = ['NK','SY','F9'] # Spirit SunCountry Frontier \n",
    "\n",
    "# Extracting for Seas:\n",
    "sea_col_fcst = ['week_x', 'week_y', 'forecastDayOfWeek','avgrasm','dowavgrasm','seats_AA_fcst', 'holiday', 'forecastId'] #+ forecastDayOfWeek, FCST\n",
    "sea_col_Cap = ['week_x', 'week_y','dow_x', 'dow_y', 'avgrasm','seats_AA_fcst','seats_OA_fcst','seats_ulcc_fcst' , 'seats_AA' , 'seats_OA' , 'seats_ulcc']\n",
    "sea_col = ['week_x', 'week_y', 'dow_x', 'dow_y','avgrasm','dowavgrasm']\n",
    "\n",
    "# Data reshaping parameters:\n",
    "train_val_percentage = .9\n",
    "time_series = False\n",
    "seasenality_one_dimension = False \n",
    "window = 0 \n",
    "\n",
    "# Model parameters;\n",
    "epochs = 150\n",
    "early_stop = 10\n",
    "sea_dense = 128\n",
    "\n",
    "\n",
    "kronos_3_timeseries = defaultdict()\n",
    "\n",
    "hcrt, mos, az = connect_to_servers()\n",
    "\n",
    "all_dest = find_all_dest_given_leg(orig , hcrt)\n",
    "all_dest.pop(0) # Remove Austin\n",
    "\n",
    "print(f\"There are {len(all_dest)} FCSTs from {orig}\")\n",
    "\n",
    "if switch_orig_dist:\n",
    "    main_dest = orig\n",
    "\n",
    "for dest in tqdm(all_dest):\n",
    "    if switch_orig_dist:\n",
    "        dest , orig = main_dest, dest\n",
    "    print(f\" Flights from {orig} to {dest}:\")\n",
    "    # Pull all the FCSTs\n",
    "    fcst_id_df = get_fcst_given_leg(orig, dest, hcrt )   \n",
    "\n",
    "    # Pull OAG:\n",
    "    oag_df = get_oag_data(orig, dest, pull_start, pull_end, ulcc_list, mos)\n",
    "\n",
    "    #Pull prdMaps:\n",
    "    prdMaps = get_prdMaps(orig, dest, hcrt)\n",
    "\n",
    "\n",
    "    # Processing: OAG Per Day:\n",
    "    oag_kl_total_Per_Day_and_AA = oag_per_day(oag_df)\n",
    "\n",
    "    all_tensors = [[] for i in range(12)]\n",
    "\n",
    "    for _,_ , fcst_id , fcst_start , fcst_end in tqdm(fcst_id_df.values):\n",
    "\n",
    "        hcrt, mos, az = connect_to_servers()\n",
    "\n",
    "        print(f\" ------------- ****** {orig}-{dest}-{fcst_id}) ****** ------------- \")\n",
    "\n",
    "        # print( fcst_id , fas, adf )\n",
    "        print( f\"fcst_start and fcst_end for {orig}-{dest} at FCST_ID {fcst_id} are: {fcst_start}, {fcst_end}\") \n",
    "\n",
    "        #  Processing: OAG per FCST:\n",
    "        oag_kl =  oag_per_fcst(oag_df, fcst_start, fcst_end )\n",
    "\n",
    "        # Merge and Normalize: OAG per FCST and OAG per Day:\n",
    "        oag_kl_fcst_total = pd.merge(oag_kl,oag_kl_total_Per_Day_and_AA ,on = \"adj_dep_date\", how='left',suffixes=('_fcst', '_day'))\n",
    "        oag_kl_fcst_total = normalize_oag_kl_fcst_total(oag_kl_fcst_total)\n",
    "\n",
    "\n",
    "        # Pull data from the file: pullData_FullPeriod.py\n",
    "        df = pull_data(orig,dest,fcst_id,new_market)\n",
    "        df = pull_seas(df, orig, dest)\n",
    "\n",
    "        if len(df) < 100:\n",
    "            print(f\"insufficent data for market ({orig}-{dest}-{fcst_id}), IGNORED\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        df['flightDepartureDate'] = pd.to_datetime(df['flightDepartureDate'], format='%Y/%m/%d')\n",
    "\n",
    "        # Merge new features (including the total day seats) into current Kronos dataset by dep_date\n",
    "        df = pd.merge(df,oag_kl_fcst_total, left_on=['flightDepartureDate'],\\\n",
    "              right_on=['adj_dep_date'], how='left')\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "        # processing: Group and pad the DF:\n",
    "        post = group_and_pad(df)\n",
    "\n",
    "        # Cut the 'post' data into Pre-covid and Post-covid parts\n",
    "        Data_PRE = post[ (post['flightDepartureDate']>=Pre_start) & (post['flightDepartureDate']<=Pre_end) ]\n",
    "        Data_POST = post[ (post['flightDepartureDate']>=Post_start) & (post['flightDepartureDate']<=Post_end) ]\n",
    "        Data_FUTURE = post[ (post['flightDepartureDate']>=future_start) & (post['flightDepartureDate']<=future_end) ]\n",
    "\n",
    "        Data_PRE = Data_PRE.reset_index(drop = True)\n",
    "        Data_POST = Data_POST.reset_index(drop = True)\n",
    "        Data_FUTURE = Data_FUTURE.reset_index(drop = True)\n",
    "\n",
    "        if test_today and not test_random_masking:\n",
    "            if DOW:\n",
    "                split_date = (datetime.strptime(test_today,'%Y-%m-%d') - timedelta(days=window*7)).strftime(\"%Y-%m-%d\")\n",
    "            else:\n",
    "                split_date = (datetime.strptime(test_today,'%Y-%m-%d') - timedelta(days=window)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "            Data_PAST = pd.concat([Data_PRE,Data_POST])\n",
    "            Data_PAST =  Data_PAST[(Data_PAST['flightDepartureDate']>=Post_start)]\n",
    "\n",
    "            Data_PRE = Data_PAST[Data_PAST['flightDepartureDate']<split_date]\n",
    "            Data_POST = Data_PAST[Data_PAST['flightDepartureDate']>=split_date]     \n",
    "\n",
    "            Data_PRE = Data_PRE.reset_index(drop = True)\n",
    "            Data_POST = Data_POST.reset_index(drop = True)\n",
    "\n",
    "        print(f\" For market ({orig}-{dest}-{fcst_id}) , we have {Data_PRE.shape[0]/14} Pre-Covid data, {Data_POST.shape[0]/14} Post-Covid data, and {Data_FUTURE.shape[0]/14} future data (from now to one year from now)\")\n",
    "\n",
    "        # TODO: This can be edited later, so we use the POST data to train....\n",
    "        if len(Data_PRE) <= len(Data_POST):\n",
    "            print(f\" *** The Pre Covid data is less than the post covid data, so we ignore {orig}-{dest}-{fcst_id} market ***\")\n",
    "            continue\n",
    "\n",
    "    #         # TODO: When merging models together this can be useefull for training porposes, but for now not usefull.\n",
    "        if len(Data_FUTURE)//14 <= 5 * 7:\n",
    "            print(f\" *** There is no future flights for the {orig}-{dest}-{fcst_id} market, so ignored! *** \")\n",
    "            continue\n",
    "\n",
    "        if any([Data_PRE.shape[0]/14 <= 10 * 7 , Data_POST.shape[0]/14 <= 10 * 7]):\n",
    "            print(f\" *** Low amount of data for either PRE, or POST of {orig}-{dest}-{fcst_id} market, so ignored! *** \")\n",
    "            continue\n",
    "\n",
    "        # MErge FCSTs:\n",
    "\n",
    "        # Train Kronos 2 (No Additional Feat (using sea_col)):\n",
    "        use_channels = True\n",
    "        seasenality_one_dimension = True\n",
    "        DOW= True\n",
    "        FC_time_series = False\n",
    "        traffic_time_series = True \n",
    "        window = 10\n",
    "        test_random_masking = False\n",
    "        # test_today = '2022-07-01'\n",
    "        train, val, test = get_train_test_samples2(Data_PRE, Data_POST, Data_FUTURE, sea_col_fcst, prdMaps , DOW= DOW , train_val_percentage = train_val_percentage ,  FC_time_series = FC_time_series , traffic_time_series = traffic_time_series,  use_channels = use_channels , seasenality_one_dimension = seasenality_one_dimension ,  window = window, test_random_masking = test_random_masking, test_today = test_today )\n",
    "\n",
    "        [all_tensors[i].append(item) for i,item in enumerate( train + val +test)]\n",
    "\n",
    "    if len(all_tensors[0]) == 0:\n",
    "        print(f\"No data for {orig}-{dest} -> No Model!\")\n",
    "\n",
    "    # Now concatinate and shuffle the data:\n",
    "    train = [np.concatenate(all_tensors[i]) for i in range(0,4)] \n",
    "    val = [np.concatenate(all_tensors[i]) for i in range(4,8)] \n",
    "    test = [np.concatenate(all_tensors[i]) for i in range(8,12)] \n",
    "\n",
    "    # Shuffle:\n",
    "    train = shuffle(train[0],train[1],train[2],train[3])\n",
    "    val = shuffle(val[0],val[1],val[2],val[3])\n",
    "\n",
    "\n",
    "    kronos32s_test_results3 , kronos32s_hist3,  kronos32s_model3 = kronos_32s_model(para_epochs = 500, para_early_stop =  True, para_model_name = 'kronos32s', para_sea_len = len(sea_col_fcst), para_sea_dense =  160, window = window ,\n",
    "             train_list = train , val_list = val, test_list = test)\n",
    "\n",
    "    _ , results_summary = test_acc(kronos32s_test_results3, test[3])\n",
    "\n",
    "    kronos_3_timeseries[f\"{orig}-{dest}\"] = results_summary\n",
    "\n",
    "    print(f\"val plot {orig}-{dest}\")\n",
    "    plt.plot(kronos32s_hist3.history['loss'])\n",
    "    plt.plot(kronos32s_hist3.history['val_loss'])\n",
    "    plt.title('loss')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','val'] , loc = \"upper left\")\n",
    "    plt.show()\n",
    "        \n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
