{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11bda252-b10d-4796-905c-c83a8b7be2b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "from pullDate_FullPeriod import pull_data , pull_seas \n",
    "\n",
    "\n",
    "# Python local connection to Oracle (herccrt) and Teradata (mosaic)\n",
    "def connect_to_servers():\n",
    "    from config import herccrt, mosaic, azure\n",
    "    hcrt = herccrt().con()\n",
    "    mos = mosaic().con()\n",
    "    az = azure().con()\n",
    "    return hcrt, mos, az\n",
    "\n",
    "# jupyter notebook settings\n",
    "import warnings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20) # DON't Use None, it will show every row --> resulting in CRASH\n",
    "\n",
    "hcrt, mos, az = connect_to_servers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6e42642-4186-4952-88f7-11ee1633e745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose directional market leg\n",
    "orig = 'PHX'\n",
    "dest = 'DFW'\n",
    "### HAS UCLL:\n",
    "# orig = 'DEN'\n",
    "# dest = 'DFW'\n",
    "\n",
    "new_market = False # change this to True if it is a new market\n",
    "\n",
    "# choose fcst_id\n",
    "fcst_id = 1\n",
    "\n",
    "\n",
    "# constants\n",
    "cabin = 'Y'\n",
    "ulcc_list = ['NK','SY','F9'] # Spirit SunCountry Frontier \n",
    "\n",
    "\n",
    "# pull data time range\n",
    "# pull_start = '2015-09-01'\n",
    "# pull_end = '2023-12-30'\n",
    "\n",
    "next_year_today = datetime.today() + timedelta(days=365)\n",
    "yesterday =  datetime.today() - timedelta(days=1)\n",
    "\n",
    "pull_start = '2017-09-01'\n",
    "# pull_end = next_year_today.strftime(\"%Y-%m-%d\")\n",
    "pull_end = yesterday.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Pre: pre-covid period, used for train and validation\n",
    "Pre_start, Pre_end = '2017-09-01', '2020-01-30'\n",
    "# Post: post-covid period, used for test\n",
    "# Post_start, Post_end = '2021-07-01', '2022-06-30'\n",
    "Post_start, Post_end = '2021-07-01', pull_end\n",
    "\n",
    "\n",
    "# # !!! update here if use another fcst_id - NOT Sure what is this!\n",
    "# fcst_start = 988\n",
    "# fcst_end = 1619"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4c77658-0a57-4da6-ba68-dd46e3934286",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_all_dest_given_leg(orig, hcrt):\n",
    "    # Find cities given one leg:\n",
    "\n",
    "    \n",
    "    fcst_id_qry = f\"\"\"\n",
    "    select Distinct LEG_DEST_S as dest\n",
    "    from fcst.fcst_id_ref\n",
    "    where LEG_ORIG_S = '{orig}'\n",
    "    \"\"\"\n",
    "    fcst_id_df = pd.read_sql_query(fcst_id_qry, con=hcrt)\n",
    "\n",
    "    return list(value[0] for value in fcst_id_df.values)\n",
    "\n",
    "\n",
    "def get_fcst_given_leg(orig, dest, hcrt ):\n",
    "    # Find fcst_id and start and end of the fcst time_bounds for a given orig dest\n",
    "\n",
    "    fcst_id_qry = f\"\"\"\n",
    "    select Distinct LEG_ORIG_S as orig, LEG_DEST_S as dest, FCST_ID as fcst_id,\n",
    "            TIME_BAND_START as time_band_start, TIME_BAND_END as time_band_end\n",
    "    from fcst.fcst_id_ref\n",
    "    where 1=1 \n",
    "    and LEG_ORIG_S = '{orig}'\n",
    "    and LEG_DEST_S = '{dest}'\n",
    "    order by 1,2,3,4\n",
    "    \"\"\"\n",
    "    fcst_id_df = pd.read_sql_query(fcst_id_qry, con=hcrt)\n",
    "   \n",
    "\n",
    "    return fcst_id_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f208fc9-5e10-40aa-a92f-45293fb9d53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oag_data(orig,dest, pull_start, pull_end):\n",
    "\n",
    "    oag_qry = f\"\"\"\n",
    "    select DEP_AIRPRT_IATA_CD as orig, \n",
    "            ARVL_AIRPRT_IATA_CD as dest,\n",
    "            LOCAL_DEP_DT as dep_date, \n",
    "            DEP_MINUTE_PAST_MDNGHT_QTY as dep_mam,\n",
    "            FLIGHT_SCHD_PUBLSH_DT as snapshot_date,\n",
    "            OPERAT_AIRLN_IATA_CD as airline, \n",
    "            OPERAT_FLIGHT_NBR as flt_id, -- Flight Number \n",
    "            EQUIP_COACH_CABIN_SEAT_QTY as seats, \n",
    "            ASMS_QTY as asm,\n",
    "            EQUIP_COACH_CABIN_SEAT_QTY * MILE_GREAT_CIRCLE_DISTANC_QTY as asm_y -- ASM for Coach Cabin\n",
    "    from PROD_INDSTR_FLIGHT_SCHD_VW.OAG_CURR  \n",
    "    where 1=1\n",
    "    and DEP_AIRPRT_IATA_CD = '{orig}'\n",
    "    and ARVL_AIRPRT_IATA_CD = '{dest}' \n",
    "    and LOCAL_DEP_DT between '{pull_start}' and '{pull_end}'\n",
    "    -- and LOCAL_DEP_DT = '2022-09-12'\n",
    "    -- and OPERAT_AIRLN_IATA_CD = 'AA'\n",
    "    and OPERAT_PAX_FLIGHT_IND = 'Y' -- new field that determines if record is a scheduled operating flight record\n",
    "    and FLIGHT_OAG_PUBLSH_CD <> 'X' -- record is active and not cancelled \n",
    "    order by 1,2,3,4,5\n",
    "    \"\"\"\n",
    "\n",
    "    oag_df = pd.read_sql(oag_qry, con=mos)\n",
    "\n",
    "\n",
    "    # convert to datetime format\n",
    "    oag_df['dep_date'] = pd.to_datetime(oag_df['dep_date'], format='%Y/%m/%d')\n",
    "\n",
    "    # convert the dep_time before 3am to the previous dep_date\n",
    "    # + Also you have ti change the dep_date for that one as well.\n",
    "    oag_df['dep_mins'] = [val+24*60 if val<180 else val for val in oag_df['dep_mam']]\n",
    "    oag_df['adj_dep_date'] = [date - dt.timedelta(days=1) if mam<180 else date for mam, date in zip(oag_df['dep_mam'],oag_df['dep_date'])]\n",
    "\n",
    "    # add yr, mo, wk cols\n",
    "    oag_df['yr'] = oag_df['adj_dep_date'].dt.year\n",
    "    oag_df['mo'] = oag_df['adj_dep_date'].dt.month\n",
    "    oag_df['wk'] = oag_df['adj_dep_date'].dt.isocalendar().week\n",
    "    # add ulcc indicator\n",
    "    oag_df['ulcc_ind'] = [1 if val in ulcc_list else 0 for val in oag_df['airline']]\n",
    "    oag_df['seats_ulcc'] = [seats if val in ulcc_list else 0 for val , seats in zip(oag_df['airline'],oag_df['seats'])]\n",
    "\n",
    "\n",
    "    return oag_df\n",
    "\n",
    "\n",
    "def get_cap_data(orig,dest, pull_start, pull_end , cabin = 'Y'):\n",
    "    cap_query = f\"\"\"\n",
    "    select LEG_DEP_AIRPRT_IATA_CD as orig, \n",
    "            LEG_ARVL_AIRPRT_IATA_CD as dest,\n",
    "            SCHD_LEG_DEP_DT as dep_date, \n",
    "            SCHD_LEG_DEP_TM as dep_time, \n",
    "            FILE_SNPSHT_DT as snapshot_date,\n",
    "            LEG_CABIN_CD as cabin, \n",
    "            OPERAT_AIRLN_IATA_CD as airline, \n",
    "            MKT_FLIGHT_NBR as flt_id,  --Flight Number\n",
    "            CABIN_CAPCTY_SEAT_QTY as seats, \n",
    "            CAB_ASM_QTY as asm, \n",
    "            CAB_TOT_RPM_QTY as rpm,\n",
    "            CAB_TOT_REVNUE_AMT as rev, \n",
    "            CAB_TOT_PAX_QTY as pax\n",
    "    from PROD_RM_BUSINES_VW.LIFE_OF_FLIGHT_LEG_CABIN \n",
    "    where 1=1\n",
    "    and FILE_SNPSHT_DT = SCHD_LEG_DEP_DT-1 -- only extract the data one day before departure\n",
    "    and LEG_DEP_AIRPRT_IATA_CD = '{orig}'\n",
    "    and LEG_ARVL_AIRPRT_IATA_CD = '{dest}'\n",
    "    and SCHD_LEG_DEP_DT between '{pull_start}' and '{pull_end}'\n",
    "    -- and SCHD_LEG_DEP_DT = '2022-09-12' \n",
    "    and LEG_CABIN_CD = '{cabin}'\n",
    "    order by 1,2,3,4,5\n",
    "    \"\"\"\n",
    "\n",
    "    cap_df = pd.read_sql(cap_query, con=mos)\n",
    "\n",
    "\n",
    "    # convert to datetime format\n",
    "    cap_df['dep_date'] = pd.to_datetime(cap_df['dep_date'], format='%Y/%m/%d')\n",
    "    # cap_df['dep_time'] = pd.to_datetime(cap_df['dep_date']+cap_df['dep_time'], format='%H:%M:%S')\n",
    "    cap_df['dep_time'] = pd.to_datetime(cap_df['dep_date'].astype(str)+' '+cap_df['dep_time'].astype(str), format='%Y/%m/%d %H:%M:%S')\n",
    "\n",
    "    # count the minutes from mid-night\n",
    "    cap_df['dep_mins'] = pd.DatetimeIndex(cap_df['dep_time']).hour*60 + pd.DatetimeIndex(cap_df['dep_time']).minute\n",
    "    # convert the dep_time before 3am to the previous dep_date\n",
    "    # + Also you have to adjust the dep_date for that one as well.\n",
    "    cap_df['adj_dep_date'] = [date - dt.timedelta(days=1) if mam<180 else date for mam, date in zip(cap_df['dep_mins'],cap_df['dep_date'])]\n",
    "    cap_df['dep_mins'] = [val+24*60 if val<180 else val for val in cap_df['dep_mins']]\n",
    "\n",
    "\n",
    "    # add yr, mo, wk\n",
    "    cap_df['yr'] = cap_df['adj_dep_date'].dt.year\n",
    "    cap_df['mo'] = cap_df['adj_dep_date'].dt.month\n",
    "    cap_df['wk'] = cap_df['adj_dep_date'].dt.isocalendar().week\n",
    "\n",
    "    return cap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f88de62-b10f-4dfd-9a87-a0afe49ebc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oag_per_day(oag_df):\n",
    "    # get OA Cap per day.\n",
    "\n",
    "    # groupby for the entire market (So we can calculate the Shares) - Sina changed it to per day.\n",
    "    gp_cols = ['adj_dep_date','airline'] \n",
    "    agg_cols = {'seats':'sum', 'asm_y':'sum','flt_id':'count','ulcc_ind':'sum', 'seats_ulcc': 'sum'}\n",
    "\n",
    "    oag_kl_Per_airline_Day = oag_df.groupby(gp_cols).agg(agg_cols).reset_index()\n",
    "\n",
    "    # change flt_id name as flt_ct, and asm_y to asm (since we are focusing on y cabin)\n",
    "    oag_kl_Per_airline_Day.rename(columns={'flt_id':'flt_ct','ulcc_ind':'ulcc_ind_mkt', 'asm_y':'asm' }, inplace=True) \n",
    "\n",
    "    # Now lets aggrigate all airlines to have infomration on all airlines.\n",
    "    gp_cols = ['adj_dep_date'] \n",
    "    agg_cols = {'seats':'sum', 'asm':'sum','flt_ct':'sum','ulcc_ind_mkt':'sum', 'seats_ulcc': 'sum'}\n",
    "\n",
    "    oag_kl_total_Per_Day = oag_kl_Per_airline_Day.groupby(gp_cols).agg(agg_cols).reset_index()\n",
    "\n",
    "    # Filter American flights into a seperate view\n",
    "\n",
    "    filter0 = oag_kl_Per_airline_Day['airline'] == 'AA'\n",
    "    oag_kl_Per_american_Day = oag_kl_Per_airline_Day[filter0]\n",
    "\n",
    "    # Drop unrelated infomration\n",
    "    oag_kl_Per_american_Day.drop(columns=['ulcc_ind_mkt','seats_ulcc','airline'],inplace=True)\n",
    "\n",
    "    # Merge the AA data with the aggrigate data \n",
    "    oag_kl_total_Per_Day_and_AA = pd.merge(oag_kl_total_Per_Day,oag_kl_Per_american_Day,on = gp_cols, how='left',suffixes=('_All', '_AA'))\n",
    "    oag_kl_total_Per_Day_and_AA.rename(columns={'ulcc_ind_mkt':'flt_ct_ulcc'}, inplace=True) \n",
    "\n",
    "    # Calculate OA data.\n",
    "    oag_kl_total_Per_Day_and_AA['seats_OA'] = oag_kl_total_Per_Day_and_AA['seats_All'] - oag_kl_total_Per_Day_and_AA['seats_AA'] -  oag_kl_total_Per_Day_and_AA['seats_ulcc'] \n",
    "    oag_kl_total_Per_Day_and_AA['flt_ct_OA'] = oag_kl_total_Per_Day_and_AA['flt_ct_All'] - oag_kl_total_Per_Day_and_AA['flt_ct_AA'] - oag_kl_total_Per_Day_and_AA['flt_ct_ulcc']\n",
    "\n",
    "    # Reformat the data\n",
    "    oag_kl_total_Per_Day_and_AA = oag_kl_total_Per_Day_and_AA.loc[:,['adj_dep_date','seats_AA','seats_OA','seats_ulcc','seats_All','flt_ct_AA' , 'flt_ct_OA' ,'flt_ct_ulcc' , 'flt_ct_All' ,'asm_AA','asm_All']]\n",
    "\n",
    "    return oag_kl_total_Per_Day_and_AA\n",
    "\n",
    "\n",
    "def oag_per_fcst(oag_df, fcst_start, fcst_end ):\n",
    "    # OAG only keep the dep_mins in the fcst_id\n",
    "    oag_df2 = oag_df[(oag_df['dep_mins']>=fcst_start) & (oag_df['dep_mins']<=fcst_end)]\n",
    "\n",
    "\n",
    "    # here we only care about AA.\n",
    "    gp_cols = ['adj_dep_date','airline']\n",
    "    agg_cols = {'seats':'sum', 'asm_y':'sum','flt_id':'count','ulcc_ind':'sum', 'seats_ulcc': 'sum' }\n",
    "\n",
    "    oag_kl = oag_df2.groupby(gp_cols).agg(agg_cols).reset_index()\n",
    "    # change flt_id name as flt_ct\n",
    "    oag_kl.rename(columns={'flt_id':'flt_ct', 'asm_y':'asm'}, inplace=True)\n",
    "    oag_kl\n",
    "    # groupby for the entire market (So we can calculate the Shares) \n",
    "    gp_cols = ['adj_dep_date'] \n",
    "    agg_cols = {'seats':'sum','asm_y':'sum' ,'flt_id':'count','ulcc_ind':'sum', 'seats_ulcc': 'sum'}\n",
    "\n",
    "    oag_kl_AAOA = oag_df2.groupby(gp_cols).agg(agg_cols).reset_index()\n",
    "    # change flt_id name as flt_ct\n",
    "    oag_kl_AAOA.rename(columns={'flt_id':'flt_ct' , 'ulcc_ind':'ulcc_count', 'asm_y':'asm' }, inplace=True) \n",
    "\n",
    "\n",
    "    # Filter the oag_df2 to show theAA Cap (per fcst span) and merge AAOA Cap (on the specific fcst).\n",
    "    filterAA = oag_kl['airline'] == 'AA'\n",
    "    oag_kl = oag_kl[filterAA]\n",
    "    oag_kl.drop(columns=['ulcc_ind','seats_ulcc','airline'],inplace=True)\n",
    "\n",
    "\n",
    "    oag_kl = pd.merge(oag_kl,oag_kl_AAOA,on = gp_cols, how='left',suffixes=('_AA_fcst', '_All_fcst'))\n",
    "\n",
    "\n",
    "    # add OA Cap\n",
    "    oag_kl['seats_OA_fcst'] = oag_kl['seats_All_fcst']-oag_kl['seats_AA_fcst']-oag_kl['seats_ulcc']\n",
    "    oag_kl['asm_OA_fcst'] = oag_kl['asm_All_fcst']-oag_kl['asm_AA_fcst']\n",
    "    oag_kl['flt_ct_OA_fcst'] = oag_kl['flt_ct_All_fcst']-oag_kl['flt_ct_AA_fcst']-oag_kl['ulcc_count']\n",
    "\n",
    "    # # add AA market share\n",
    "    # oag_kl['seats_share'] = oag_kl['seats_AA_fcst']/oag_kl['seats_AAOA']\n",
    "    # oag_kl['asm_share'] = oag_kl['asm_AA_fcst']/oag_kl['asm_AAOA']\n",
    "    # oag_kl['flt_ct_share'] = oag_kl['flt_ct_AA_fcst']/oag_kl['flt_ct_AAOA']\n",
    "    # # add seats_per_flt\n",
    "    # oag_kl['seats_per_flt_AA'] = oag_kl['seats_AA']/oag_kl['flt_ct_AA']\n",
    "    # oag_kl['seats_per_flt_OA'] = oag_kl['seats_OA']/oag_kl['flt_ct_OA']\n",
    "    # oag_kl['seats_per_flt_AAOA'] = oag_kl['seats_AAOA']/oag_kl['flt_ct_AAOA']\n",
    "\n",
    "    oag_kl['fcst_start'] = fcst_start\n",
    "    oag_kl['fcst_end'] = fcst_end\n",
    "    oag_kl.rename(columns={'seats_ulcc':'seats_ulcc_fcst' , 'ulcc_count':'flt_ct_ulcc_fcst' }, inplace=True) \n",
    "\n",
    "    oag_kl = oag_kl.loc[:,['adj_dep_date', 'fcst_start', 'fcst_end','seats_AA_fcst','seats_OA_fcst','seats_ulcc_fcst','seats_All_fcst','flt_ct_AA_fcst' , 'flt_ct_OA_fcst' ,'flt_ct_ulcc_fcst' , 'flt_ct_All_fcst' ,'asm_AA_fcst','asm_All_fcst']]\n",
    "\n",
    "    return oag_kl\n",
    "\n",
    "def normalize_oag_kl_fcst_total(oag_kl_fcst_total): \n",
    "    \n",
    "    # Drop Null\n",
    "    oag_kl_fcst_total.dropna(inplace=True)\n",
    "\n",
    "    # # remove the \n",
    "    # oag_kl_fcst_total.drop(columns=['seats','asm','flt_ct' , 'fcst_start' , 'fcst_end'],inplace=True)\n",
    "\n",
    "    # Normalize Cap features\n",
    "    norm_cols = [\n",
    "        'seats_AA_fcst',\n",
    "        'seats_OA_fcst', 'seats_ulcc_fcst', 'seats_All_fcst', 'flt_ct_AA_fcst',\n",
    "        'flt_ct_OA_fcst', 'flt_ct_ulcc_fcst', 'flt_ct_All_fcst', 'asm_AA_fcst',\n",
    "        'asm_All_fcst', 'seats_AA', 'seats_OA', 'seats_ulcc', 'seats_All',\n",
    "        'flt_ct_AA', 'flt_ct_OA', 'flt_ct_ulcc', 'flt_ct_All', 'asm_AA',\n",
    "        'asm_All']\n",
    "\n",
    "    oag_kl_fcst_total[norm_cols] = minmax_scale(oag_kl_fcst_total[norm_cols])\n",
    "    \n",
    "    return oag_kl_fcst_total\n",
    "\n",
    "\n",
    "def aa_cap_fcst(cap_df,fcst_start, fcst_end ):\n",
    "    # AA Cap only keep the dep_mins in the fcst_id\n",
    "    cap_df2 = cap_df[(cap_df['dep_mins']>=fcst_start) & (cap_df['dep_mins']<=fcst_end)]\n",
    "\n",
    "    # get AA Cap\n",
    "    gp_cols = ['dep_date']   \n",
    "    agg_cols = {'seats':'sum','asm':'sum','flt_id':'count','rpm':'sum','rev':'sum','pax':'sum'}\n",
    "\n",
    "    cap_kl = cap_df2.groupby(gp_cols).agg(agg_cols).reset_index()\n",
    "    cap_kl.rename(columns={'flt_id':'flt_ct'}, inplace=True)\n",
    "\n",
    "    # add other Cap features\n",
    "    cap_kl['rasm'] = cap_kl['rev']/cap_kl['asm']\n",
    "    cap_kl['yield'] = cap_kl['rev']/cap_kl['rpm']\n",
    "    cap_kl['load_fac'] = cap_kl['rpm']/cap_kl['asm']\n",
    "\n",
    "    # replace N/A with 0\n",
    "    # print(cap_kl.isnull().sum())\n",
    "    cap_kl = cap_kl.replace(np.nan, 0)\n",
    "    # print(cap_kl.isnull().sum())\n",
    "\n",
    "    return cap_kl\n",
    "\n",
    "\n",
    "\n",
    "def merge_oag_aacap(oag_kl,cap_kl): \n",
    "    # merge OAG and AA Cap data\n",
    "\n",
    "    oag_kl.rename(columns={'adj_dep_date':'dep_date'}, inplace=True)\n",
    "\n",
    "    oag_cap_kl = pd.merge(cap_kl,oag_kl,on=['dep_date'],how='left',suffixes=('_cap', '_oag'))\n",
    "\n",
    "    # print(oag_cap_kl.isnull().sum())\n",
    "    oag_cap_kl.dropna(inplace=True)\n",
    "\n",
    "    # remove the \n",
    "    oag_cap_kl.drop(columns=['seats','asm','flt_ct' , 'fcst_start' , 'fcst_end'],inplace=True)\n",
    "\n",
    "    # Normalize Cap features\n",
    "    norm_cols = ['rpm', 'rev', 'pax', 'rasm',\n",
    "           'yield', 'load_fac', 'seats_AA_fcst',\n",
    "           'seats_OA_fcst', 'seats_ulcc_fcst', 'seats_All_fcst', 'flt_ct_AA_fcst',\n",
    "           'flt_ct_OA_fcst', 'flt_ct_ulcc_fcst', 'flt_ct_All_fcst', 'asm_AA_fcst',\n",
    "           'asm_All_fcst']\n",
    "\n",
    "    oag_cap_kl[norm_cols] = minmax_scale(oag_cap_kl[norm_cols])\n",
    "    \n",
    "    return oag_cap_kl\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e24919e-4b6d-4bd0-b01d-4d2059ac68e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_group_id(df):\n",
    "    \"\"\"\n",
    "    Identify and padding the unique group\n",
    "    Each group contains 14 rows (local/flow, 7 fcst_perd) x 10 cols (frac_closure).\n",
    "    Each flight is a group!\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    df = df.sort_values(['snapshotDate','origin','destination',\n",
    "                                'forecastId','flightDepartureDate','forecastDayOfWeek','poolCode',\n",
    "                                 'cabinCode','localFlowIndicator'])\n",
    "\n",
    "    # 'GroupBy.cumcount': Number each item in each group from 0 to (the length of that group - 1).\n",
    "    # '== 0' returns True or False\n",
    "    # '.astype(int)' ,convert True/False to 1/0\n",
    "    df['groupID'] = (df.groupby(['snapshotDate','origin','destination',\n",
    "                                'forecastId','flightDepartureDate','forecastDayOfWeek','poolCode',\n",
    "                                 'cabinCode']).cumcount() == 0).astype(int)\n",
    "    # assign each unique group a new group id\n",
    "    # groupID==1 will be a new id (+1), groupID==0 will indicate the same id for the same group\n",
    "    df['groupID'] = df['groupID'].cumsum()\n",
    "\n",
    "    # Full Hisxtory Pre Fixing\n",
    "    # count the num of 'forecastPeriod' in each group\n",
    "    # because some group might not have all 7 'forecastPeriod'\n",
    "    df['fullHistory'] = df.groupby(['groupID'])[\"forecastPeriod\"].transform(\"count\")\n",
    "    # Indicator it is part of history and not a pad\n",
    "    df['real'] = 1\n",
    "    return df\n",
    "\n",
    "\n",
    "def empty_group():\n",
    "    # Creating an empty group to be used for padding.\n",
    "    fullKeysArray = np.zeros( (14, 50) ) # 'fullKeys' uses 50 columns \n",
    "\n",
    "    # frac_closure = 0: fully closed with 0 traffic\n",
    "    for i in range(0,fullKeysArray.shape[0]): \n",
    "        if i <= 6: # Flow: 0 ~ 6\n",
    "            fullKeysArray[i][0] = 1 # 'localFlowIndicator' column\n",
    "            fullKeysArray[i][1] = i + 1 # 'forecastPeriod' column: 1 ~ 7\n",
    "            fullKeysArray[i][2:12] = 1 # 'fracClosure' column all set to 1 \n",
    "        else: # Local: 7 ~ 13s\n",
    "            fullKeysArray[i][0] = 0 # 'localFlowIndicator' column\n",
    "            fullKeysArray[i][1] = i - 6 # 'forecastPeriod' column: 1 ~ 7\n",
    "            fullKeysArray[i][2:12] = 1 # 'fracClosure' column: all set to 1 \n",
    "\n",
    "    fullKeys = pd.DataFrame(fullKeysArray)        \n",
    "    fullKeys.columns = ['localFlowIndicator','forecastPeriod'] + [f\"fracClosure_{i}\" for i in range(1,11)] + [f\"trafficActual_{i}\" for i in range(1,11)] + [f\"trafficActualAadv_{i}\" for i in range(1,11)] + ['holiday','H1','H2','H3','HL','weekNumber','week_x','week_y','dow_x','dow_y','avgtraffic','avgtrafficopenness','avgrasm','dowavgtraffic','dowavgtrafficopenness','dowavgrasm','groupID','fullHistory']\n",
    "    fullKeys['localFlowIndicator'] = ['F' if lfi == 1 else 'L' for lfi in fullKeys['localFlowIndicator']]\n",
    "    return fullKeys\n",
    "\n",
    "def padding_groups(df):\n",
    "    fullKeys = empty_group()\n",
    "    \n",
    "    groupbyColumns = ['snapshotDate','origin','destination','forecastId','flightDepartureDate',\n",
    "                  'forecastDayOfWeek','poolCode','cabinCode']\n",
    "    grouped = df.groupby(groupbyColumns)\n",
    "    \n",
    "    merged_list = []\n",
    "    count_rows_misskey = 0\n",
    "    for g in grouped:\n",
    "\n",
    "        # g[0] is the directory key and g[1] is the value (actual data)\n",
    "        # identify the cells of g[1] that not in fullKeys\n",
    "        # g[1][~g[1].isin(fullKeys)] \n",
    "\n",
    "        # identify the missing keys in g[1]\n",
    "        key = g[1][['localFlowIndicator','forecastPeriod']]\n",
    "        missingKeys = fullKeys[~fullKeys[['localFlowIndicator','forecastPeriod']].apply(tuple,1).isin(key.apply(tuple,1))]\n",
    "        count_rows_misskey += missingKeys.shape[0]\n",
    "\n",
    "        # append the missing keys under the data\n",
    "        fullHistory = pd.concat([g[1],missingKeys])\n",
    "\n",
    "        # use 0 to indicate padding data\n",
    "        fullHistory['real'].fillna(0, inplace=True) \n",
    "\n",
    "        # fill the data with missing keys\n",
    "        fullHistory = fullHistory.fillna(method='ffill')\n",
    "        merged_list.append(fullHistory)\n",
    "\n",
    "    # merge all data across 'flightDepartureDate'   \n",
    "    out = pd.concat(merged_list)\n",
    "    out = out.sort_values(['snapshotDate','origin','destination',\n",
    "                                'forecastId','flightDepartureDate','forecastDayOfWeek','poolCode',\n",
    "                                 'cabinCode','localFlowIndicator'])\n",
    "    \n",
    "    \n",
    "    # Get full history and then concat fake history (padding) from above \n",
    "    post = out.copy()\n",
    "    post = post.sort_values(['snapshotDate','origin','destination',\n",
    "                                'forecastId','flightDepartureDate','forecastDayOfWeek','poolCode',\n",
    "                                 'cabinCode','localFlowIndicator'])\n",
    "\n",
    "\n",
    "    post['groupID'] = (post.groupby(['snapshotDate','origin','destination',\n",
    "                                'forecastId','flightDepartureDate','forecastDayOfWeek','poolCode',\n",
    "                                 'cabinCode']).cumcount()==0).astype(int)\n",
    "    post['groupID'] = post['groupID'].cumsum()\n",
    "    # Full Hisotyr Pre Fixing\n",
    "    post['fullHistory'] = post.groupby(['groupID'])[\"forecastPeriod\"].transform(\"count\")\n",
    "\n",
    "    post['flightDepartureDate'] = pd.to_datetime(post['flightDepartureDate'], format='%Y/%m/%d')\n",
    "\n",
    "    \n",
    "    return post\n",
    "\n",
    "def group_and_pad(df):\n",
    "    return padding_groups(create_group_id(df))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1afbb95-7b5e-4ce3-a771-eb68d44ac2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensors(DataFarame, sea_col_Cap, test = False, time_series = True , use_channels = False , seasenality_one_dimension = True ,  window = 10):\n",
    "    \n",
    "    len_sea_cap = len(sea_col_Cap)\n",
    "            \n",
    "    \n",
    "    # fractional closure\n",
    "    PRE_FC_L = DataFarame[['fracClosure_' + str(i + 1) for i in range(10)]].values.astype('float32')\n",
    "    # seasonality\n",
    "    PRE_Sea_L = DataFarame[sea_col_Cap].values.astype('float32')\n",
    "    # actual traffic\n",
    "    if test:\n",
    "        Traffic = False\n",
    "    else:\n",
    "        PRE_Traf_L = DataFarame[['trafficActual_' + str(i + 1) for i in range(10)]].values.astype('float32')\n",
    "    \n",
    "    \n",
    "    # reshape the data for CNNLSTM model\n",
    "    FC = PRE_FC_L.reshape(int(PRE_FC_L.shape[0]/14), 1, 14, 10)\n",
    "    Seasenality = PRE_Sea_L.reshape(int(PRE_Sea_L.shape[0]/14), 1,14, len_sea_cap)\n",
    "    if not test:\n",
    "        Traffic = PRE_Traf_L.reshape(int(PRE_Traf_L.shape[0]/14), 1, 14, 10)\n",
    "    \n",
    "    # Remove Duplicates (from 2d to 1d vector)\n",
    "    if seasenality_one_dimension:\n",
    "        Seasenality =np.delete(Seasenality, slice(13), 2).reshape(Seasenality.shape[0],len_sea_cap)\n",
    "    \n",
    "    \n",
    "    if use_channels:\n",
    "        FC = FC.reshape(len(FC),2,7,10)\n",
    "        if not test:\n",
    "            Traffic = Traffic.reshape(len(Traffic),2,7,10)\n",
    "    \n",
    "    # Change FC shape to refelect time series:\n",
    "    # print(FC.shape)\n",
    "    if time_series:\n",
    "        time_series_widow= list()\n",
    "        Seasenality_times = list()\n",
    "        for i in range(window,len(FC)):\n",
    "            # print(FC[i-window:i].shape)\n",
    "            time_series_widow.append(FC[i-window:i].reshape(window,2,7,10))\n",
    "            # print((Seasenality[i-window:i].shape))\n",
    "            Seasenality_times.append(Seasenality[i-window:i])\n",
    "        FC = np.array(time_series_widow)\n",
    "        Seasenality = np.array(Seasenality_times)\n",
    "        \n",
    "        # Since the 1st window size data points are removed:\n",
    "        # Seasenality = Seasenality[window:]\n",
    "        if not test:\n",
    "            Traffic = Traffic[window:]\n",
    "        \n",
    "        \n",
    "    return FC, Seasenality, Traffic\n",
    "   \n",
    "    \n",
    "    \n",
    "def get_train_test_samples(Data_PRE, Data_POST, Data_FUTURE, sea_col_Cap , train_val_percentage = 0.9 , time_series = False, use_channels = False,  seasenality_one_dimension = True , window = 0  ):\n",
    "    \n",
    "    PRE_FC , PRE_Seas , PRE_Traf  = get_tensors(Data_PRE, sea_col_Cap , test = False, time_series =time_series , use_channels = use_channels , seasenality_one_dimension=seasenality_one_dimension , window =window)\n",
    "    POST_FC , POST_Seas , _  = get_tensors(Data_POST, sea_col_Cap, test = True, time_series = time_series, use_channels = use_channels, seasenality_one_dimension=seasenality_one_dimension , window = window )\n",
    "    # TODO: FOR NOW WE ARE NOT USING THE FUTURE:\n",
    "    FUTURE_FC , FUTURE_Seas , _  = get_tensors(Data_FUTURE, sea_col_Cap ,test = True, time_series = time_series, use_channels = use_channels, seasenality_one_dimension=seasenality_one_dimension , window = window)\n",
    "    \n",
    "    # 80, 20 --> \n",
    "    # TODO: THIS SHOULD BE CHANGED TO RANDOMIZED.\n",
    "    train_val_cutoff = round(PRE_FC.shape[0]*train_val_percentage) \n",
    "\n",
    "\n",
    "    # prepare train/val/test datasets\n",
    "    PRE_FC_train = PRE_FC[:train_val_cutoff, :]\n",
    "    PRE_FC_val = PRE_FC[train_val_cutoff:, :]\n",
    "\n",
    "    PRE_Seas_train = PRE_Seas[:train_val_cutoff, :]\n",
    "    PRE_Seas_val = PRE_Seas[train_val_cutoff:, :]\n",
    "\n",
    "    PRE_Traf_train = PRE_Traf[:train_val_cutoff, :]\n",
    "    PRE_Traf_val = PRE_Traf[train_val_cutoff:, :]\n",
    "\n",
    "\n",
    "\n",
    "    train = [PRE_FC_train,PRE_Seas_train,PRE_Traf_train]\n",
    "    val = [PRE_FC_val,PRE_Seas_val,PRE_Traf_val]\n",
    "    test = [POST_FC,POST_Seas]\n",
    "    \n",
    "    return train, val, test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d40a58b-e01b-4d07-98b7-9c8fed05c7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randPeriod(prdMaps):\n",
    "    # print(x)\n",
    "    random_period = np.random.randint(1,7)\n",
    "    rrd_start, rrt_end = prdMaps[prdMaps['FORECASTPERIOD']==random_period].loc[:,[\"RRD_START\", \"RRD_END\"]].values[0]\n",
    "    random_day = np.random.randint(rrd_start, rrt_end )\n",
    "    return random_period , random_day\n",
    "\n",
    "\n",
    "def tf_timeseries_masking( tf_tensors , data_index , prdMaps , window  ):\n",
    "    \"\"\"This function will generate masked time-seried terrafic data.\"\"\"\n",
    "    \n",
    "    random_period , random_day_to_dept = randPeriod(prdMaps)\n",
    "    # print(random_period , random_day_to_dept )\n",
    "    arr =  prdMaps.iloc[:,3].values\n",
    "    \n",
    "    # output = tf_tensors[data_index].copy()\n",
    "    test_tensors = tf_tensors.copy()\n",
    "    test_tensors[data_index][:,:random_period,] = -1\n",
    "    \n",
    "    \n",
    "    max_bond_period = random_day_to_dept\n",
    "    min_bond_period = arr[random_period-1]\n",
    "    remaining_window = window - 1\n",
    "    current_index = data_index\n",
    "    max_min_range = max_bond_period - min_bond_period\n",
    "    current_period = random_period\n",
    "\n",
    "    if  max_min_range < remaining_window:\n",
    "        while max_min_range  <= remaining_window:\n",
    "            # print(current_index-max_min_range,current_index)\n",
    "            test_tensors[current_index-max_min_range:current_index,:,:current_period,] = -1\n",
    "            current_period -= 1\n",
    "            if current_period == 0:\n",
    "                break\n",
    "            current_index -= max_min_range\n",
    "            remaining_window -=  max_min_range\n",
    "            max_bond_period -= max_min_range\n",
    "            min_bond_period = arr[current_period-1]\n",
    "            max_min_range = max_bond_period - min_bond_period\n",
    "            # reaching Today date:\n",
    "\n",
    "    if max_min_range >= remaining_window:\n",
    "        # print(current_index-max_min_range,current_index)\n",
    "        test_tensors[current_index-remaining_window:current_index,:,:current_period,] = -1\n",
    "    \n",
    "    return test_tensors[data_index+1-window:data_index+1] \n",
    "\n",
    "\n",
    "def get_tensors2(DataFarame, sea_col_Cap, prdMaps= None , FC_time_series = True , traffic_time_series = False ,  use_channels = False , seasenality_one_dimension = True ,  window = 10, DOW=False):\n",
    "    \n",
    "    len_sea_cap = len(sea_col_Cap)\n",
    "            \n",
    "    \n",
    "    # fractional closure\n",
    "    PRE_FC_L = DataFarame[['fracClosure_' + str(i + 1) for i in range(10)]].values.astype('float32')\n",
    "    # seasonality\n",
    "    PRE_Sea_L = DataFarame[sea_col_Cap].values.astype('float32')\n",
    "    # actual traffic\n",
    "    PRE_Traf_L = DataFarame[['trafficActual_' + str(i + 1) for i in range(10)]].values.astype('float32')\n",
    "    \n",
    "    \n",
    "    # reshape the data for CNNLSTM model\n",
    "    FC = PRE_FC_L.reshape(int(PRE_FC_L.shape[0]/14), 1, 14, 10)\n",
    "    Seasenality = PRE_Sea_L.reshape(int(PRE_Sea_L.shape[0]/14), 1,14, len_sea_cap)\n",
    "    Traffic = PRE_Traf_L.reshape(int(PRE_Traf_L.shape[0]/14), 1, 14, 10)\n",
    "    \n",
    "    # Remove Duplicates (from 2d to 1d vector)\n",
    "    if seasenality_one_dimension:\n",
    "        Seasenality =np.delete(Seasenality, slice(13), 2).reshape(Seasenality.shape[0],len_sea_cap)\n",
    "    \n",
    "    \n",
    "    if use_channels:\n",
    "        FC = FC.reshape(len(FC),2,7,10)\n",
    "        Traffic = Traffic.reshape(len(Traffic),2,7,10)\n",
    "    \n",
    "    # Change FC shape to refelect time series:\n",
    "    # print(FC.shape)\n",
    "    if FC_time_series:\n",
    "        time_series_widow= list()\n",
    "        Seasenality_times = list()\n",
    "        for i in range(window,len(FC)):\n",
    "            # print(FC[i-window:i].shape)\n",
    "            time_series_widow.append(FC[i-window:i].reshape(window,2,7,10))\n",
    "            # print((Seasenality[i-window:i].shape))\n",
    "            Seasenality_times.append(Seasenality[i-window:i])\n",
    "        FC = np.array(time_series_widow)\n",
    "        Seasenality = np.array(Seasenality_times)\n",
    "        \n",
    "        # Since the 1st window size data points are removed:\n",
    "        # Seasenality = Seasenality[window:]\n",
    "        Traffic = Traffic[window:]\n",
    "    \n",
    "    elif traffic_time_series:\n",
    "        traffic_time_series_window = list()\n",
    "        Seasenality_times = list()\n",
    "        for i in range(window,len(Traffic)):\n",
    "            # Find Random period and random day:\n",
    "            if DOW:\n",
    "                tf_window_masked = tf_timeseries_masking_DOW( Traffic  , i , prdMaps , window  )\n",
    "            else:\n",
    "                tf_window_masked = tf_timeseries_masking( Traffic  , i , prdMaps , window  )\n",
    "            traffic_time_series_window.append(tf_window_masked)\n",
    "            # Seasenality_times.append(Seasenality[i-window:i])\n",
    "        TF_time = np.array(traffic_time_series_window)\n",
    "        # Seasenality = np.array(Seasenality_times)\n",
    "        Seasenality = Seasenality[window:]\n",
    "        FC = FC[window:]\n",
    "        \n",
    "        Traffic = Traffic[window:]\n",
    "            \n",
    "        return FC, Seasenality, Traffic, TF_time\n",
    "    \n",
    "        \n",
    "    return FC, Seasenality, Traffic, _\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def floorSearch(arr, low, high, x):\n",
    " \n",
    "    # If low and high cross each other\n",
    "    if (low > high):\n",
    "        return -1\n",
    " \n",
    "    # If last element is smaller than x\n",
    "    if (x >= arr[high]):\n",
    "        return high\n",
    " \n",
    "    # Find the middle point\n",
    "    mid = int((low + high) / 2)\n",
    " \n",
    "    # If middle point is floor.\n",
    "    if (arr[mid] == x):\n",
    "        return mid\n",
    " \n",
    "    # If x lies between mid-1 and mid\n",
    "    if (mid > 0 and arr[mid-1] <= x\n",
    "            and x < arr[mid]):\n",
    "        return mid - 1\n",
    " \n",
    "    # If x is smaller than mid,\n",
    "    # floor must be in left half.\n",
    "    if (x < arr[mid]):\n",
    "        return floorSearch(arr, low, mid-1, x)\n",
    " \n",
    "    # If mid-1 is not floor and x is greater than\n",
    "    # arr[mid],\n",
    "    return floorSearch(arr, mid + 1, high, x)\n",
    "\n",
    "\n",
    "def tf_timeseries_masking_DOW( tf_tensors , data_index , prdMaps , window  ):\n",
    "    \"\"\"This function will generate masked time-seried terrafic data, and is based on DOW.\"\"\"\n",
    "    \n",
    "    random_period , random_day_to_dept = randPeriod(prdMaps)\n",
    "    arr =  prdMaps.iloc[:,3].values\n",
    "    test_tensors = tf_tensors.copy()\n",
    "    \n",
    "    day_to_dept = random_day_to_dept\n",
    "    current_index = data_index\n",
    "    \n",
    "    for i in range(0,window):\n",
    "        # Move back 7 days in each iter.\n",
    "        day_to_dept = random_day_to_dept - i*7 \n",
    "        # Get the period of that day to dept.\n",
    "        flrs = floorSearch(arr, 0, 6, day_to_dept)\n",
    "        current_period = flrs+1\n",
    "        # If we get today, will break the loop. and use all the values (no masking)\n",
    "        if current_period == 0:\n",
    "            break\n",
    "        # mask the values\n",
    "        test_tensors[current_index,:,:current_period,] = -1\n",
    "        \n",
    "        # Update index:\n",
    "        current_index -= 1\n",
    "        \n",
    "    return test_tensors[data_index+1-window:data_index+1]    \n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "# def dow_get_tensors2(DataFarame , sea_col_Cap, prdMaps= None  ,  test = False, time_series = True,  use_channels = False , window = 10):   \n",
    "def dow_get_tensors2(DataFarame , sea_col_Cap, prdMaps= None  ,  FC_time_series = True , traffic_time_series = False ,  use_channels = False , seasenality_one_dimension = True ,  window = 10):\n",
    "    DOW = True\n",
    "    FC_dow , Seasenality_dow, Traffic_dow ,  TF_time_dow  = list(), list(), list(), list()\n",
    "    \n",
    "    for i in DataFarame.loc[ :,\t['forecastDayOfWeek' ]].drop_duplicates().values:\n",
    "        # filter_y = DataFarame['dow_y' ] == i[1] \n",
    "        # filter_x = DataFarame['dow_x'] == i[0] \n",
    "        filter_dow =  DataFarame['forecastDayOfWeek'] == i[0] \n",
    "        # print(filter_y.shape , filter_x.shape)\n",
    "        # print(i)\n",
    "        Data_dow =DataFarame[filter_dow]\n",
    "        # print(Data_dow.shape)\n",
    "        FC, Seasenality, Traffic, TF_time= get_tensors2(Data_dow, sea_col_Cap, prdMaps  , FC_time_series  , traffic_time_series ,  use_channels  , seasenality_one_dimension  ,  window, DOW )\n",
    "        FC_dow.append(FC)\n",
    "        Seasenality_dow.append(Seasenality)\n",
    "        Traffic_dow.append(Traffic)\n",
    "        if traffic_time_series:\n",
    "            TF_time_dow.append(TF_time)\n",
    "    \n",
    "    # Then Concat together, now each datapoint is based on DOW.\n",
    "    FC_dow = [ i  for i in FC_dow if i.shape!=(0,)]\n",
    "    Seasenality_dow = [ i  for i in Seasenality_dow if i.shape!=(0,)]\n",
    "    # Traffic_dow = [ i  for i in Traffic_dow if i.shape!=(0,)]\n",
    "    \n",
    "    if traffic_time_series:\n",
    "        TF_time_dow = [ i  for i in TF_time_dow if i.shape!=(0,)]\n",
    "        TF_time_dow = np.concatenate(TF_time_dow)\n",
    "    \n",
    "    FC_dow = np.concatenate(FC_dow)\n",
    "    Seasenality_dow = np.concatenate(Seasenality_dow)\n",
    "    Traffic_dow = np.concatenate(Traffic_dow)\n",
    "    \n",
    "    return FC_dow , Seasenality_dow, Traffic_dow , TF_time_dow\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_prdMaps(orig, dest, hcrt):\n",
    "    prdMaps = pd.read_sql(f\"\"\"select DISTINCT leg_orig as origin, leg_dest as destination, fcst_period as forecastPeriod, rrd_band_start_i as rrd_start, rrd_band_end_i as rrd_end\n",
    "                            from market_xref a \n",
    "                            join FCST.FCST_PERIOD_REF b \n",
    "                            on a.infl_period_id = b.FCST_PERIOD_ID\n",
    "                            where 1=1 \n",
    "                            and cabin_code = 'Y'\n",
    "                            and leg_orig = '{orig}'\n",
    "                            and leg_dest = '{dest}'\n",
    "                            ORDER BY forecastPeriod\n",
    "                            \"\"\", con = hcrt)\n",
    "    return prdMaps\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_train_test_samples2(Data_PRE, Data_POST, Data_FUTURE, sea_col_Cap, prdMaps , DOW= False , train_val_percentage = 0.9  ,  FC_time_series = True , traffic_time_series = False ,  use_channels = False , seasenality_one_dimension = True ,  window = 10 ):\n",
    "    \n",
    "    \n",
    "    \n",
    "    if DOW:\n",
    "        PRE_FC , PRE_Seas , PRE_Traf, PRE_TF_timeseries  =       dow_get_tensors2(Data_PRE , sea_col_Cap, prdMaps  , FC_time_series = FC_time_series , traffic_time_series = traffic_time_series ,  use_channels = use_channels , seasenality_one_dimension = seasenality_one_dimension ,   window = window)\n",
    "        POST_FC , POST_Seas , POST_Traf , POST_TF_timeseries =  dow_get_tensors2(Data_POST , sea_col_Cap, prdMaps  , FC_time_series = FC_time_series , traffic_time_series = traffic_time_series ,  use_channels = use_channels , seasenality_one_dimension = seasenality_one_dimension ,   window = window)\n",
    "        # FUTURE_FC , FUTURE_Seas , FUTURE_Traf ,FUTUR_TF_timeseries = dow_get_tensors2(Data_FUTURE , sea_col_Cap, prdMaps  , FC_time_series = False , traffic_time_series = True ,  use_channels = True , seasenality_one_dimension = True ,   window = window)\n",
    "\n",
    "\n",
    "    else:\n",
    "        PRE_FC , PRE_Seas , PRE_Traf , PRE_TF_timeseries =      get_tensors2(Data_PRE, sea_col_Cap, prdMaps  , FC_time_series = FC_time_series , traffic_time_series = traffic_time_series ,  use_channels = use_channels , seasenality_one_dimension = seasenality_one_dimension ,   window = window)\n",
    "        POST_FC , POST_Seas , POST_Traf , POST_TF_timeseries = get_tensors2(Data_POST, sea_col_Cap, prdMaps , FC_time_series = FC_time_series , traffic_time_series = traffic_time_series ,  use_channels = use_channels , seasenality_one_dimension = seasenality_one_dimension ,   window = window)\n",
    "        # FUTURE_FC , FUTURE_Seas , FUTURE_Traf , FUTURE_TF_timeseries = get_tensors2(Data_FUTURE, sea_col_Cap, prdMaps , FC_time_series = False , traffic_time_series = True ,  use_channels = True , seasenality_one_dimension = True ,   window = window)\n",
    "\n",
    "    # Train/Val Spilit:\n",
    "    # TODO: THIS SHOULD BE CHANGED TO RANDOMIZED.\n",
    "    train_val_cutoff = round(PRE_FC.shape[0]*train_val_percentage) \n",
    "\n",
    "\n",
    "    # prepare train/val/test datasets\n",
    "    PRE_FC_train = PRE_FC[:train_val_cutoff, :]\n",
    "    PRE_FC_val = PRE_FC[train_val_cutoff:, :]\n",
    "\n",
    "    PRE_Seas_train = PRE_Seas[:train_val_cutoff, :]\n",
    "    PRE_Seas_val = PRE_Seas[train_val_cutoff:, :]\n",
    "\n",
    "    PRE_Traf_train = PRE_Traf[:train_val_cutoff, :]\n",
    "    PRE_Traf_val = PRE_Traf[train_val_cutoff:, :]\n",
    "\n",
    "    PRE_TF_timeseries_train = PRE_TF_timeseries[:train_val_cutoff, :]\n",
    "    PRE_TF_timeseries_val = PRE_TF_timeseries[train_val_cutoff:, :]\n",
    "\n",
    "\n",
    "\n",
    "    train = [PRE_FC_train,PRE_Seas_train,PRE_TF_timeseries_train,PRE_Traf_train]\n",
    "    val = [PRE_FC_val,PRE_Seas_val,PRE_TF_timeseries_val,PRE_Traf_val]\n",
    "    test = [POST_FC , POST_Seas ,  POST_TF_timeseries , POST_Traf]\n",
    "    \n",
    "    return train, val, test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ee1029-7a16-4dc8-a29d-7e8955052ac5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "955c0e47-c899-4f20-83c1-ea4e1f2aa84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kronos Model\n",
    "import os\n",
    "from keras import backend as K \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Reshape , BatchNormalization, LSTM, Concatenate, Dense, Activation, Flatten, Conv2D, Conv1D, ConvLSTM2D, Conv3D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, History, ModelCheckpoint\n",
    "import random\n",
    "\n",
    "\n",
    "def kronos_32s_model(para_epochs, para_early_stop, para_model_name, para_sea_len, para_sea_dense, window, train_list, val_list, test_list):\n",
    "        \n",
    "    # Set Random Seed \n",
    "    seed_value = 44\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "\n",
    "    \n",
    "    # extract train/val/test datasets  ** since TF.keras only accepts channels_last we have to move axix **\n",
    "    train_fc, train_sea, train_traf_time, train_output = train_list\n",
    "    val_fc, val_sea, val_traf_time, val_output = val_list\n",
    "    test_fc, test_sea, test_traf_time, test_gold_output = test_list\n",
    "    \n",
    "    # Reshape for Conv3d with channel 1.\n",
    "    train_fc, val_fc, test_fc = train_fc.reshape(list(train_fc.shape)+[1]), val_fc.reshape(list(val_fc.shape)+[1]), test_fc.reshape(list(test_fc.shape)+[1]) \n",
    "    \n",
    "    \n",
    "    # Model Parameters:\n",
    "    batch_size = 100\n",
    "    early_stop = 100\n",
    "    filter_num = 8\n",
    "    rec_act = 'relu'\n",
    "    rec_dropout = 0.1\n",
    "    dropout = 0.1\n",
    "    lstm_size = 64\n",
    "    patience= early_stop\n",
    "    batch_size_nbr = batch_size\n",
    "    Error_Method = 'MC_dropout'\n",
    "    B = batch_size\n",
    "    # 128, 64,64\n",
    "\n",
    "    # Input sizes: \n",
    "    K.clear_session()\n",
    "    input_tensor_closure = Input(shape = (2,7,10,1)) # lets use 3D with Channel of 1\n",
    "    input_tensor_seas = Input(shape = (window ,para_sea_len))\n",
    "    input_tensor_seas = Input(shape = (para_sea_len))\n",
    "    input_tensor_trrafic_tseries = Input(shape = (window ,2,7,10))\n",
    "\n",
    "    # Feed the TF-t-series to ConvLSTM:\n",
    "    cnn_lstm_tf_time = ConvLSTM2D(filters = filter_num , kernel_size = 3, padding = 'same', data_format = 'channels_first', \n",
    "                                  recurrent_activation = rec_act, recurrent_dropout = rec_dropout, dropout=dropout ,  return_sequences = False)(input_tensor_trrafic_tseries)\n",
    "    cnn_lstm_tf_time = BatchNormalization(axis = 1)(cnn_lstm_tf_time)\n",
    "    # cnn_lstm_tf_time = ConvLSTM2D(filters = filter_num//2 , kernel_size = 2, padding = 'same', data_format = 'channels_first', \n",
    "    #                               recurrent_activation = rec_act, recurrent_dropout = rec_dropout, dropout=dropout ,  return_sequences = False)(cnn_lstm_tf_time)\n",
    "    # cnn_lstm_tf_time = BatchNormalization(axis = 1)(cnn_lstm_tf_time)\n",
    "    cnn_lstm_tf_time = Flatten()(cnn_lstm_tf_time)\n",
    "    cnn_lstm_tf_time = Dense(32, activation=rec_act)(cnn_lstm_tf_time)\n",
    "\n",
    "    # print(cnn_lstm_tf_time)\n",
    "\n",
    "\n",
    "    # # layer for seasonality Using a RNN (Here GRU instead of LSTM)\n",
    "    # rnn_seas = GRU(lstm_size, recurrent_dropout = rec_dropout, dropout=dropout  , return_sequences = False  )(input_tensor_seas)\n",
    "    # # rnn_seas = Bidirectional(GRU(lstm_size//2, recurrent_dropout = rec_dropout, dropout=dropout  , return_sequences = False ))(rnn_seas)\n",
    "    # rnn_seas = BatchNormalization()(rnn_seas)\n",
    "    # rnn_seas = Dense(64, activation=rec_act)(rnn_seas)\n",
    "\n",
    "    # Seas No RNN:\n",
    "    rnn_seas = Dense(128, activation=rec_act)(input_tensor_seas)\n",
    "    rnn_seas = Dense(32, activation=rec_act)(rnn_seas)\n",
    "\n",
    "\n",
    "    # print(rnn_seas)\n",
    "\n",
    "\n",
    "    # layer for FC:\n",
    "    cnn_FC = Conv3D(filters = filter_num , kernel_size = 3, strides = (1, 1, 1), padding = 'same', data_format = 'channels_last')(input_tensor_closure)\n",
    "    cnn_FC = Conv3D(filters = filter_num//2 , kernel_size = 3, strides = (1, 1, 1), padding = 'same', data_format = 'channels_last')(cnn_FC)\n",
    "    cnn_FC = BatchNormalization()(cnn_FC)\n",
    "    cnn_FC = Flatten()(cnn_FC)\n",
    "    cnn_FC = Dense(32, activation=rec_act)(cnn_FC)\n",
    "\n",
    "    # print(cnn_FC)\n",
    "\n",
    "\n",
    "    # concat frac_closure and seasonality and TF_sries\n",
    "    FF_concat_all = Concatenate()([cnn_lstm_tf_time, rnn_seas, cnn_FC])\n",
    "    # FF_concat_all = Dense(256, activation=rec_act)(FF_concat_all)\n",
    "    FF_concat_all = Dense(140, activation=rec_act)(FF_concat_all)\n",
    "\n",
    "    # # Output format:\n",
    "    output_tensor = Reshape((2, 7, 10))(FF_concat_all)\n",
    "\n",
    "    # print(output_tensor)\n",
    "\n",
    "\n",
    "    # Kronos model setting\n",
    "    Kronos_model = Model(inputs = [ input_tensor_closure, input_tensor_seas, input_tensor_trrafic_tseries], outputs = output_tensor)\n",
    "    Kronos_model.compile(loss = 'mean_squared_error', optimizer = 'adam')\n",
    "    \n",
    "    if para_early_stop: # with early stop\n",
    "        \n",
    "        os.chdir(r\"\\\\corpaa.aa.com\\campusshared\\HDQ\\HDQ_REVMGMT_Share\\RMDEPT\")\n",
    "        path = \"BFox/Kronos/Prototype/Output/FullPeriod/\"\n",
    "        market_nm = orig + dest\n",
    "        # if not os.path.exists(path):\n",
    "            # os.mkdir(path)\n",
    "        \n",
    "        # history = History()\n",
    "        metric = 'val_accuracy'\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=patience , restore_best_weights=True)\n",
    "        mc = ModelCheckpoint(path + '//' + market_nm + '_' + 'best_model.h5', monitor='val_loss', mode='auto', verbose=0, save_best_only=True, save_freq='epoch')\n",
    "        base_hist = Kronos_model.fit([train_fc, train_sea, train_traf_time], train_output, epochs = para_epochs, \n",
    "                                 batch_size = batch_size_nbr, validation_data = ([ val_fc, val_sea, val_traf_time], val_output), verbose = 0, callbacks=[es,mc])\n",
    "    else: # no early stop\n",
    "        base_hist = Kronos_model.fit([train_fc, train_sea, train_traf_time], train_output, epochs = para_epochs, \n",
    "                                 batch_size = batch_size_nbr, validation_data = ([ val_fc, val_sea, val_traf_time], val_output) , verbose = 0)\n",
    "         \n",
    "    \n",
    "    # use the trained model to predict\n",
    "    test_pred = Kronos_model.predict([ test_fc, test_sea, test_traf_time])\n",
    "\n",
    "    return test_pred, base_hist, Kronos_model\n",
    "\n",
    "\n",
    "def test_acc(prediction_results, gold_labels):\n",
    "    results = pd.DataFrame()\n",
    "    test_size = gold_labels.shape[0]\n",
    "\n",
    "    gold_tr_reshaped = gold_labels.reshape(test_size*14,10)\n",
    "    pred_tr_reshaped = kronos32s_test_results3.reshape(test_size*14,10)\n",
    "\n",
    "    # Gold, top, mid, bot\n",
    "    results['gold_top_tr'] = gold_tr_reshaped[:,:3].sum(1)\n",
    "    results['gold_mid_tr'] = gold_tr_reshaped[:,3:7].sum(1)\n",
    "    results['gold_bot_tr'] = gold_tr_reshaped[:,7:].sum(1)\n",
    "    results['gold_sum_tr'] = gold_tr_reshaped.sum(1)\n",
    "\n",
    "    # Pred Top, Mid, Bot\n",
    "    results['pred_top_tr'] = pred_tr_reshaped[:,:3].sum(1)\n",
    "    results['pred_mid_tr'] = pred_tr_reshaped[:,3:7].sum(1)\n",
    "    results['pred_bot_tr'] = pred_tr_reshaped[:,7:].sum(1)\n",
    "    results['pred_sum_tr'] = pred_tr_reshaped.sum(1)\n",
    "\n",
    "    # all FvT errors\n",
    "    results['top_FvT'] = results['pred_top_tr'] - results['gold_top_tr']\n",
    "    results['mid_FvT'] = results['pred_mid_tr'] - results['gold_mid_tr']\n",
    "    results['bot_FvT'] = results['pred_bot_tr'] - results['gold_bot_tr']\n",
    "    results['sum_FvT'] = results['pred_sum_tr'] - results['gold_sum_tr']\n",
    "\n",
    "    # squared FvT errors for MSE\n",
    "    results['top_FvT_sqr'] = results['top_FvT']**2\n",
    "    results['mid_FvT_sqr'] = results['mid_FvT']**2\n",
    "    results['bot_FvT_sqr'] = results['bot_FvT']**2\n",
    "    results['sum_FvT_sqr'] = results['sum_FvT']**2\n",
    "    \n",
    "    result_sum = { \n",
    "                    \"top_FvT\" : [results['top_FvT'].mean(), results['top_FvT'].std(), results['top_FvT_sqr'].mean()],\n",
    "                    \"mid_FvT\" : [results['mid_FvT'].mean(), results['mid_FvT'].std(), results['mid_FvT_sqr'].mean()],\n",
    "                    \"bot_FvT\" : [results['bot_FvT'].mean(), results['bot_FvT'].std(), results['bot_FvT_sqr'].mean()],\n",
    "                    \"sum_FvT\" : [results['sum_FvT'].mean(), results['sum_FvT'].std(), results['sum_FvT_sqr'].mean()]\n",
    "                }\n",
    "    return results , result_sum\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f671dc97-70fe-40cf-b4fd-2382a2e47116",
   "metadata": {},
   "source": [
    "## Training Loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005f82bf-d347-478d-982e-1e00c92eabf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def store_data(data, path, file_name):\n",
    "    save_path = os.path.join(path, f\"{file_name}.pickle\" )\n",
    "    with open(save_path, 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c992996-13c4-4e3e-bb4e-5ce53e6400d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring the warnings\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Ignoring the warnings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05762174-7243-49ff-bcfd-36648dc1117b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11 FCSTs from LAS\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c64aed15edd24818be37d2b503cbb335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Flights from LAS to CLT:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c43869061441179135220199458638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------------- ****** LAS-CLT-1) ****** ------------- \n",
      "fcst_start and fcst_end for LAS-CLT at FCST_ID 1 are: 180, 431\n",
      " For market (LAS-CLT-1) , we have 816.0 Pre-Covid data, 474.0 Post-Covid data, and 210.0 future data (from now to one year from now)\n",
      " ------------- ****** LAS-CLT-2) ****** ------------- \n",
      "fcst_start and fcst_end for LAS-CLT at FCST_ID 2 are: 432, 548\n",
      " For market (LAS-CLT-2) , we have 577.0 Pre-Covid data, 77.0 Post-Covid data, and 0.0 future data (from now to one year from now)\n",
      " *** There is no future flights for the LAS-CLT-2 market, so ignored! *** \n",
      " ------------- ****** LAS-CLT-3) ****** ------------- \n",
      "fcst_start and fcst_end for LAS-CLT at FCST_ID 3 are: 549, 637\n",
      " For market (LAS-CLT-3) , we have 348.0 Pre-Covid data, 454.0 Post-Covid data, and 196.0 future data (from now to one year from now)\n",
      " ------------- ****** LAS-CLT-4) ****** ------------- \n",
      "fcst_start and fcst_end for LAS-CLT at FCST_ID 4 are: 638, 798\n",
      " For market (LAS-CLT-4) , we have 284.0 Pre-Covid data, 469.0 Post-Covid data, and 207.0 future data (from now to one year from now)\n",
      " ------------- ****** LAS-CLT-5) ****** ------------- \n",
      "fcst_start and fcst_end for LAS-CLT at FCST_ID 5 are: 799, 1099\n",
      " For market (LAS-CLT-5) , we have 645.0 Pre-Covid data, 318.0 Post-Covid data, and 187.0 future data (from now to one year from now)\n",
      " ------------- ****** LAS-CLT-6) ****** ------------- \n",
      "fcst_start and fcst_end for LAS-CLT at FCST_ID 6 are: 1100, 1420\n",
      " For market (LAS-CLT-6) , we have 753.0 Pre-Covid data, 376.0 Post-Covid data, and 205.0 future data (from now to one year from now)\n",
      " ------------- ****** LAS-CLT-7) ****** ------------- \n",
      "fcst_start and fcst_end for LAS-CLT at FCST_ID 7 are: 1421, 1619\n",
      " For market (LAS-CLT-7) , we have 708.0 Pre-Covid data, 366.0 Post-Covid data, and 223.0 future data (from now to one year from now)\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "switch_orig_dist = False\n",
    "orig = \"PHX\"\n",
    "\n",
    "yesterday =  datetime.today() - timedelta(days=2)\n",
    "next_year_today = datetime.today() + timedelta(days=365)\n",
    "\n",
    "pull_start = '2017-09-01'\n",
    "pull_end = next_year_today.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Pre: pre-covid period, used for train and validation\n",
    "Pre_start, Pre_end = '2017-09-01', '2020-01-30'\n",
    "# Post: post-covid period, used for test\n",
    "Post_start, Post_end = '2021-07-01',  yesterday.strftime(\"%Y-%m-%d\")\n",
    "# Future: Today till one year in future:\n",
    "future_start , future_end = datetime.today().strftime(\"%Y-%m-%d\") ,   next_year_today.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "new_market= False\n",
    "ulcc_list = ['NK','SY','F9'] # Spirit SunCountry Frontier \n",
    "\n",
    "# Extracting for Seas:\n",
    "sea_col_fcst = ['week_x', 'week_y', 'forecastDayOfWeek','avgrasm','dowavgrasm', 'forecastId'] #+ forecastDayOfWeek, FCST\n",
    "sea_col_Cap = ['week_x', 'week_y','dow_x', 'dow_y', 'avgrasm','seats_AA_fcst','seats_OA_fcst','seats_ulcc_fcst' , 'seats_AA' , 'seats_OA' , 'seats_ulcc']\n",
    "sea_col = ['week_x', 'week_y', 'dow_x', 'dow_y','avgrasm','dowavgrasm']\n",
    "\n",
    "# Data reshaping parameters:\n",
    "train_val_percentage = .9\n",
    "time_series = False\n",
    "seasenality_one_dimension = False \n",
    "window = 0 \n",
    "\n",
    "# Model parameters;\n",
    "epochs = 150\n",
    "early_stop = 10\n",
    "sea_dense = 128\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "kronos_3_timeseries = defaultdict()\n",
    "\n",
    "hcrt, mos, az = connect_to_servers()\n",
    "\n",
    "\n",
    "# Pull all the dest given rig.\n",
    "all_dest = find_all_dest_given_leg(orig , hcrt)\n",
    "# all_dest.pop(0) # Remove Austin\n",
    "\n",
    "print(f\"There are {len(all_dest)} FCSTs from {orig}\")\n",
    "\n",
    "if switch_orig_dist:\n",
    "    main_dest = orig\n",
    "\n",
    "for dest in tqdm(all_dest):\n",
    "    if switch_orig_dist:\n",
    "        dest , orig = main_dest, dest\n",
    "    print(f\" Flights from {orig} to {dest}:\")\n",
    "    # Pull all the FCSTs\n",
    "    fcst_id_df = get_fcst_given_leg(orig, dest, hcrt )   \n",
    "    \n",
    "    if len(fcst_id_df) == 0:\n",
    "        print(f\" *** Fot the {orig}-{dest}, there is no avilable FCSTs! *** \")\n",
    "        continue \n",
    "    # Pull OAG:\n",
    "    oag_df = get_oag_data(orig,dest, pull_start, pull_end)\n",
    "    \n",
    "    #Pull prdMaps:\n",
    "    prdMaps = get_prdMaps(orig, dest, hcrt)\n",
    "    \n",
    "\n",
    "    # Processing: OAG Per Day:\n",
    "    oag_kl_total_Per_Day_and_AA = oag_per_day(oag_df)\n",
    "    \n",
    "    all_tensors = [[] for i in range(12)]\n",
    "    \n",
    "    for _,_ , fcst_id , fcst_start , fcst_end in tqdm(fcst_id_df.values):\n",
    "        \n",
    "        hcrt, mos, az = connect_to_servers()\n",
    "        \n",
    "        print(f\" ------------- ****** {orig}-{dest}-{fcst_id}) ****** ------------- \")\n",
    "        \n",
    "        # print( fcst_id , fas, adf )\n",
    "        print( f\"fcst_start and fcst_end for {orig}-{dest} at FCST_ID {fcst_id} are: {fcst_start}, {fcst_end}\") \n",
    "        \n",
    "        #  Processing: OAG per FCST:\n",
    "        oag_kl =  oag_per_fcst(oag_df, fcst_start, fcst_end )\n",
    "        \n",
    "        # Merge and Normalize: OAG per FCST and OAG per Day:\n",
    "        oag_kl_fcst_total = pd.merge(oag_kl,oag_kl_total_Per_Day_and_AA ,on = \"adj_dep_date\", how='left',suffixes=('_fcst', '_day'))\n",
    "        oag_kl_fcst_total = normalize_oag_kl_fcst_total(oag_kl_fcst_total)\n",
    "        \n",
    "        \n",
    "        # Pull data from the file: pullData_FullPeriod.py\n",
    "        df = pull_data(orig,dest,fcst_id,new_market)\n",
    "        df = pull_seas(df, orig, dest)\n",
    "        \n",
    "        if len(df) < 100:\n",
    "            print(f\"insufficent data for market ({orig}-{dest}-{fcst_id}), IGNORED\")\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        df['flightDepartureDate'] = pd.to_datetime(df['flightDepartureDate'], format='%Y/%m/%d')\n",
    "\n",
    "        # Merge new features (including the total day seats) into current Kronos dataset by dep_date\n",
    "        df = pd.merge(df,oag_kl_fcst_total, left_on=['flightDepartureDate'],\\\n",
    "              right_on=['adj_dep_date'], how='left')\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # processing: Group and pad the DF:\n",
    "        post = group_and_pad(df)\n",
    "        \n",
    "        # Cut the 'post' data into Pre-covid and Post-covid parts\n",
    "        Data_PRE = post[ (post['flightDepartureDate']>=Pre_start) & (post['flightDepartureDate']<=Pre_end) ]\n",
    "        Data_POST = post[ (post['flightDepartureDate']>=Post_start) & (post['flightDepartureDate']<=Post_end) ]\n",
    "        Data_FUTURE = post[ (post['flightDepartureDate']>=future_start) & (post['flightDepartureDate']<=future_end) ]\n",
    "        \n",
    "        print(f\" For market ({orig}-{dest}-{fcst_id}) , we have {Data_PRE.shape[0]/14} Pre-Covid data, {Data_POST.shape[0]/14} Post-Covid data, and {Data_FUTURE.shape[0]/14} future data (from now to one year from now)\")\n",
    "        \n",
    "        # TODO: This can be edited later, so we use the POST data to train....\n",
    "        if len(Data_PRE) <= len(Data_POST):\n",
    "            print(f\" *** The Pre Covid data is less than the post covid data, so we ignore {orig}-{dest}-{fcst_id} market ***\")\n",
    "            continue\n",
    "        \n",
    "#         # TODO: When merging models together this can be useefull for training porposes, but for now not usefull.\n",
    "        if len(Data_FUTURE)//14 <= 5 * 7:\n",
    "            print(f\" *** There is no future flights for the {orig}-{dest}-{fcst_id} market, so ignored! *** \")\n",
    "            continue\n",
    "        \n",
    "        if any([Data_PRE.shape[0]/14 <= 10 * 7 , Data_POST.shape[0]/14 <= 10 * 7]):\n",
    "            print(f\" *** Low amount of data for either PRE, or POST of {orig}-{dest}-{fcst_id} market, so ignored! *** \")\n",
    "            continue\n",
    "                \n",
    "        # MErge FCSTs:\n",
    "            \n",
    "        # Train Kronos 2 (No Additional Feat (using sea_col)):\n",
    "        use_channels = True\n",
    "        seasenality_one_dimension = True\n",
    "        DOW= True\n",
    "        FC_time_series = False\n",
    "        traffic_time_series = True \n",
    "        window = 10\n",
    "        train, val, test = get_train_test_samples2(Data_PRE, Data_POST, Data_FUTURE, sea_col_fcst, prdMaps , DOW= DOW , train_val_percentage = train_val_percentage ,  FC_time_series = FC_time_series , traffic_time_series = traffic_time_series,  use_channels = use_channels , seasenality_one_dimension = seasenality_one_dimension ,  window = window )\n",
    "        \n",
    "        [all_tensors[i].append(item) for i,item in enumerate( train + val +test)]\n",
    "        \n",
    "    if len(all_tensors[0]) == 0:\n",
    "        print(f\"No data for {orig}-{dest} -> No Model!\")\n",
    "        continue\n",
    "    # Now concatinate and shuffle the data:\n",
    "    train = [np.concatenate(all_tensors[i]) for i in range(0,4)] \n",
    "    val = [np.concatenate(all_tensors[i]) for i in range(4,8)] \n",
    "    test = [np.concatenate(all_tensors[i]) for i in range(8,12)] \n",
    "    \n",
    "    # Shuffle:\n",
    "    train = shuffle(train[0],train[1],train[2],train[3])\n",
    "    val = shuffle(val[0],val[1],val[2],val[3])\n",
    "    \n",
    "    \n",
    "    kronos32s_test_results3 , kronos32s_hist3,  kronos32s_model3 = kronos_32s_model(para_epochs = 500, para_early_stop =  True, para_model_name = 'kronos32s', para_sea_len = len(sea_col_fcst), para_sea_dense =  160, window = window ,\n",
    "             train_list = train , val_list = val, test_list = test)\n",
    "\n",
    "    _ , results_summary = test_acc(kronos32s_test_results3, test[3])\n",
    "\n",
    "    kronos_3_timeseries[f\"{orig}-{dest}\"] = results_summary\n",
    "\n",
    "    print(f\"val plot {orig}-{dest}\")\n",
    "    plt.plot(kronos32s_hist3.history['loss'])\n",
    "    plt.plot(kronos32s_hist3.history['val_loss'])\n",
    "    plt.title('loss')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','val'] , loc = \"upper left\")\n",
    "    plt.show()\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "store_data(kronos_3_timeseries, \".\", \"kronos3_results\")        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
